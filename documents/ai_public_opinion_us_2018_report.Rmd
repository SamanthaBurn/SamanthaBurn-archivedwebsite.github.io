---
title: 'The 2018 U.S. AI Public Opinion Survey'
output:
  pdf_document:
    toc: true
    toc_depth: 4
  html_notebook:
    toc: true
    toc_depth: 4
  html_document:
    toc: true
    toc_depth: 4
bibliography: us_2018_bib.bib
author: "Baobao Zhang and Allan Dafoe"
date: "June 27, 2018"
---

# Introduction

This report is based on findings from a nationally representative survey conducted by the Governance of AI Program at the University of Oxford using the survey firm YouGov. The survey was conducted between June 6 and June 14, 2018; 2,000 American adults (18+) completed the survey. The Methodology section in the Appendix provides further details regarding the details of the data collection and analysis process. [^1]

[^1]: Unless otherwise specified: Heteroscedasticity-consistent standard errors are reported in parentheses after point estimates. In figures, each error bar shows the 95% confidence intervals. Each confidence ellipse shows the 95% confidence region of the bivariate means assuming the two variables are distributed multivariate normal.

## Acknowledgments

We would like to thank the following for giving us useful feedback on this project: Michael Page, Jeffrey Ding, Brian Tse, Remco Zwetsloot, and the YouGov Team (Marissa Shih and Sam Luks).

# Executive summary

This report explores Americans' attitudes towards artificial intelligence (AI), an emerging technology that could impact many aspects of society. We highlight some key findings from our study:

- In Americans’ minds, the terms AI, machine learning, automation, and robotics have distinct meanings. The public’s understanding of these terms has nuance, but is often inaccurate. For instance, a majority of Americans do not think Facebook photo tagging, Google Search, Netflix or Amazon recommendations, or Google Translate use AI or machine learning.
- Americans express a mixed opinion about developing AI. A substantial minority of Americans (41%) support developing AI while a smaller minority (22%) oppose developing AI.
- Demographic subgroups that indicate greater support for developing AI include younger Americans, males, Democrats, those with high income, those with computer science or programming experience, and those who do not identify as Christian. 
- An overwhelming majority of Americans (82%) agree that robots and/or AI should be carefully managed. Americans' response is on par with that of European respondents. 
- Americans consider all major governance challenges associated with AI to be important for governments and tech companies to manage. Governance challenges perceived to be the most likely to impact Americans within the next decade and most important include:
    1. protecting data privacy
    2. preventing AI cyber attacks against governments, companies, organizations, and individuals
    3. preventing AI-assisted surveillance from violating privacy and civil liberties
    4. preventing AI from being used to spread fake and harmful content online
- Americans do not have a great deal of confidence in the U.S. government, international organizations, corporations or other actors to develop or manage AI in the best interest of the public. The U.S. military and university researchers are perceived to be the most trusted groups to develop AI. The public places the most trust in Partnership on AI and non-governmental scientific organizations (e.g., AAAI) to manage the development and use of AI.
- Americans, in general, are more supportive of the U.S. investing more in AI military capabilities _and_ of cooperating with China to avoid the dangers of an AI arms race. Providing respondents with information about the risks of a U.S.-China AI arms race decreases support for the U.S. investing more in AI military capabilities by 27%. But other experimental information treatments fail to change Americans' policy preferences.
- Respondents predict that high-level machine intelligence will arrive sooner than AI researchers forecast. The median respondent predicts there is 54% probability of high-level machine intelligence by 2028, 70% probability by 2038, and 88% probability by 2068.
- Americans express mixed support for developing high-level machine intelligence. About one-third of Americans (32%) somewhat or strongly support the development of AI while 27% somewhat or strongly oppose the development of AI.
- Americans are ambivalent about high-level machine intelligence, and lean towards thinking it will be on balance harmful. Twenty-two percent think the technology will be “on balance bad," and 12% that it would be "extremely bad, possibly human extinction." By contrast, 21% think it will be "on balance good," and 5% think it will be "extremely good." 

```{r setup, include=FALSE, cache=TRUE, warning=FALSE}
rm(list = ls(all = TRUE))

# Load packages
library(haven)
library(Hmisc)
library(magrittr)
library(dplyr)
library(ggplot2)
library(car)
library(labelled)
library(stringr)
library(scales)
library(pander)
library(knitr)
library(tidyr)
library(stargazer)
library(ggrepel)
library(questionr)
library(miceadds)
library(kableExtra)

# Robust Standard Error Function
summary.lm <- function (object,
                        correlation = FALSE,
                        symbolic.cor = FALSE,
                        robust = FALSE,
                        cluster = c(NULL, NULL),
                        ...) {
  # add extension for robust standard errors
  if (robust == TRUE) {
    # save variable that are necessary to calcualte robust sd
    X <- model.matrix(object)
    u2 <- residuals(object) ^ 2
    XDX <- 0
    
    ## One needs to calculate X'DX. But due to the fact that
    ## D is huge (NxN), it is better to do it with a cycle.
    for (i in 1:nrow(X)) {
      XDX <- XDX + u2[i] * X[i, ] %*% t(X[i, ])
    }
    
    # inverse(X'X)
    XX1 <- solve(t(X) %*% X, tol = 1e-100)
    
    # Sandwich Variance calculation (Bread x meat x Bread)
    varcovar <- XX1 %*% XDX %*% XX1
    
    # adjust degrees of freedom
    dfc_r <- sqrt(nrow(X)) / sqrt(nrow(X) - ncol(X))
    
    # Standard errors of the coefficient estimates are the
    # square roots of the diagonal elements
    rstdh <- dfc_r * sqrt(diag(varcovar))
  }
  # add extension for clustered standard errors
  if (!is.null(cluster) &
      robust == T) {
    warning(
      "Robust standard errors are calculated. Set robust=F to calculate clustered standard errors."
    )
  }
  if (!is.null(cluster) & robust == F) {
    if ("" %in% cluster) {
      stop("No variable for clustering provided.")
    }
    if (length(cluster) > 2) {
      stop("The function only allows max. 2 clusters. You provided more.")
    }
    n_coef <- all.vars(object$call$formula)
    if (length(cluster) == 1) {
      dat <- na.omit(get(paste(object$call$data))[, c(n_coef, cluster)])
      if (nrow(dat) < nrow(object$model)) {
        stop("Not all observation have a cluster.")
      }
      cluster <- dat[, cluster]
      require(sandwich, quietly = TRUE)
      M <- res_length <- length(unique(cluster))
      N <- length(cluster)
      K <- object$rank
      dfc <- (M / (M - 1)) * ((N - 1) / (N - K))
      uj  <-
        na.omit(apply(estfun(object), 2, function(x)
          tapply(x, cluster, sum)))
      
      varcovar <- dfc * sandwich(object, meat = crossprod(uj) / N)
      rstdh <- sqrt(diag(varcovar))
    }
    if (length(cluster) == 2) {
      dat_1 <-
        na.omit(get(paste(object$call$data))[, c(n_coef, cluster[1])])
      if (nrow(dat_1) < nrow(object$model)) {
        stop("Not all observation have a cluster.")
      }
      dat_2 <-
        na.omit(get(paste(object$call$data))[, c(n_coef, cluster[2])])
      if (nrow(dat_2) < nrow(object$model)) {
        stop("Not all observation have a cluster.")
      }
      
      dat <-
        na.omit(get(paste(object$call$data))[, c(n_coef, cluster)])
      library(sandwich, quietly = TRUE)
      cluster1 <- dat[, cluster[1]]
      cluster2 <- dat[, cluster[2]]
      cluster12 = paste(cluster1, cluster2, sep = "")
      M1  <- length(unique(cluster1))
      M2  <- length(unique(cluster2))
      M12 <- res_length <- length(unique(cluster12))
      N   <- length(cluster1)
      K   <- object$rank
      dfc1  <- (M1 / (M1 - 1)) * ((N - 1) / (N - K))
      dfc2  <- (M2 / (M2 - 1)) * ((N - 1) / (N - K))
      dfc12 <- (M12 / (M12 - 1)) * ((N - 1) / (N - K))
      u1j   <-
        apply(estfun(object), 2, function(x)
          tapply(x, cluster1,  sum))
      u2j   <-
        apply(estfun(object), 2, function(x)
          tapply(x, cluster2,  sum))
      u12j  <-
        apply(estfun(object), 2, function(x)
          tapply(x, cluster12, sum))
      vc1   <-  dfc1 * sandwich(object, meat = crossprod(u1j) / N)
      vc2   <-  dfc2 * sandwich(object, meat = crossprod(u2j) / N)
      vc12  <- dfc12 * sandwich(object, meat = crossprod(u12j) / N)
      varcovar <- vc1 + vc2 - vc12
      rstdh <- sqrt(diag(varcovar))
    }
    
  }
  z <- object
  p <- z$rank
  rdf <- z$df.residual
  if (p == 0) {
    r <- z$residuals
    n <- length(r)
    w <- z$weights
    if (is.null(w)) {
      rss <- sum(r ^ 2)
    }
    else {
      rss <- sum(w * r ^ 2)
      r <- sqrt(w) * r
    }
    resvar <- rss / rdf
    ans <- z[c("call", "terms", if (!is.null(z$weights))
      "weights")]
    class(ans) <- "summary.lm"
    ans$aliased <- is.na(coef(object))
    ans$residuals <- r
    ans$df <- c(0L, n, length(ans$aliased))
    ans$coefficients <- matrix(NA, 0L, 4L)
    dimnames(ans$coefficients) <- list(NULL, c("Estimate",
                                               "Std. Error", "t value", "Pr(>|t|)"))
    ans$sigma <- sqrt(resvar)
    ans$r.squared <- ans$adj.r.squared <- 0
    return(ans)
  }
  if (is.null(z$terms))
    stop("invalid 'lm' object:  no 'terms' component")
  if (!inherits(object, "lm"))
    ac
  warning("calling summary.lm(<fake-lm-object>) ...")
  Qr <- stats:::qr.lm(object)
  n <- NROW(Qr$qr)
  if (is.na(z$df.residual) || n - p != z$df.residual)
    warning("residual degrees of freedom in object suggest this is not an \"lm\" fit")
  r <- z$residuals
  f <- z$fitted.values
  w <- z$weights
  if (is.null(w)) {
    mss <- if (attr(z$terms, "intercept"))
      sum((f - mean(f)) ^ 2)
    else
      sum(f ^ 2)
    rss <- sum(r ^ 2)
  }
  else {
    mss <- if (attr(z$terms, "intercept")) {
      m <- sum(w * f / sum(w))
      sum(w * (f - m) ^ 2)
    }
    else
      sum(w * f ^ 2)
    rss <- sum(w * r ^ 2)
    r <- sqrt(w) * r
  }
  resvar <- rss / rdf
  if (is.finite(resvar) && resvar < (mean(f) ^ 2 + var(f)) *
      1e-30)
    warning("essentially perfect fit: summary may be unreliable")
  p1 <- 1L:p
  R <- chol2inv(Qr$qr[p1, p1, drop = FALSE])
  se <- sqrt(diag(R) * resvar)
  
  if (robust == T) {
    se <- rstdh
  }
  if (!is.null(cluster) & robust == F) {
    se <- rstdh
  }
  est <- z$coefficients[Qr$pivot[p1]]
  tval <- est / se
  ans <- z[c("call", "terms", if (!is.null(z$weights))
    "weights")]
  ans$residuals <- r
  pval <- 2 * pt(abs(tval),
                 rdf, lower.tail = FALSE)
  ans$coefficients <- cbind(est, se, tval, pval)
  dimnames(ans$coefficients) <-
    list(names(z$coefficients)[Qr$pivot[p1]],
         c("Estimate", "Std. Error", "t value", "Pr(>|t|)"))
  ans$aliased <- is.na(coef(object))
  ans$sigma <- sqrt(resvar)
  ans$df <- c(p, rdf, NCOL(Qr$qr))
  if (p != attr(z$terms, "intercept")) {
    df.int <- if (attr(z$terms, "intercept"))
      1L
    else
      0L
    ans$r.squared <- mss / (mss + rss)
    ans$adj.r.squared <- 1 - (1 - ans$r.squared) * ((n -
                                                       df.int) / rdf)
    ans$fstatistic <- c(
      value = (mss / (p - df.int)) / resvar,
      numdf = p - df.int,
      dendf = rdf
    )
    if (robust == T | (!is.null(cluster))) {
      if (!is.null(cluster)) {
        rdf <- res_length - 1
      }
      pos_coef <- match(names(z$coefficients)[-match("(Intercept)",
                                                     names(z$coefficients))],
                        names(z$coefficients))
      
      P_m <- matrix(z$coefficients[pos_coef])
      
      R_m <- diag(1,
                  length(pos_coef),
                  length(pos_coef))
      
      ans$fstatistic <- c(
        value = t(R_m %*% P_m) %*%
          (solve(varcovar[pos_coef, pos_coef], tol = 1e-100)) %*%
          (R_m %*% P_m) / (p - df.int),
        numdf = p - df.int,
        dendf = rdf
      )
      
    }
    
  }
  else
    ans$r.squared <- ans$adj.r.squared <- 0
  ans$cov.unscaled <- R
  dimnames(ans$cov.unscaled) <- dimnames(ans$coefficients)[c(1,
                                                             1)]
  if (correlation) {
    ans$correlation <- (R * resvar) / outer(se, se)
    dimnames(ans$correlation) <- dimnames(ans$cov.unscaled)
    ans$symbolic.cor <- symbolic.cor
  }
  if (!is.null(z$na.action))
    ans$na.action <- z$na.action
  class(ans) <- "summary.lm"
  ans
}

# Text wrapper function
wrapper <- function(x, ...) {
  paste(strwrap(x, ...), collapse = "\n")
}

# Function to create bivariate mean confidence region
bivCI <- function(s, xbar, n, alpha, m) {
  x <- sin(2 * pi * (0:(m - 1)) / (m - 1))
  y <- cos(2 * pi * (0:(m - 1)) / (m - 1))
  cv <- qchisq(1 - alpha, 2)
  cv <- cv / n
  for (i in 1:m) {
    pair <- c(x[i], y[i])
    q <- pair %*% solve(s, pair)
    x[i] <- x[i] * sqrt(cv / q) + xbar[1]
    y[i] <- y[i] * sqrt(cv / q) + xbar[2]
  }
  data.frame(x, y)
}

# Trailing zeros
roundfunc <- function(x,
                      round_digits = 2,
                      lessthan = TRUE) {
  if (lessthan) {
    temp <- ifelse(x > 0 & round(x, round_digits) == 0,
                   paste0("<0.", rep(0, (round_digits - 1)), 1),
                   sprintf(paste0("%.", round_digits, "f"), round(x, round_digits)))
    temp <- ifelse(x < 0 & round(x, round_digits) == 0,
                   paste0(">-0.", rep(0, (round_digits - 1)), 1),
                   temp)
    return(temp)
  } else {
    return(sprintf(paste0("%.", round_digits, "f"), round(x, round_digits)))
  }
}

roundfunc(x = 0.001, round_digits = 2)
relabel_var <- function(old_var, old_labels, new_labels) {
  new_var <- rep(NA, length(old_var))
  if (is.factor(old_var)) {
    old_var <- as.character(old_var)
  }
  for (i in 1:length(old_labels)) {
    new_var[old_var == old_labels[i]] <- new_labels[i]
  }
  return(new_var)
}

relabel_var2 <- function(old_var, old_labels, new_labels) {
  new_var <- rep(NA, length(old_var))
  if (is.factor(old_var)) {
    old_var <- as.character(old_var)
  }
  for (i in 1:length(old_labels)) {
    new_var[old_var %in% old_labels[[i]]] <- new_labels[i]
  }
  return(new_var)
}

catvar_func <-
  function(outcome,
           outcome_var,
           label_var,
           num_missing,
           num_DK,
           shown,
           output_type,
           new_values,
           survey_weights = d$survey_weights,
           missing_recode) {
    # Clean data to make the bar chart
    # Get the value labels
    value_labels <- as.data.frame(val_labels(label_var))
    value_labels$labels <- row.names(value_labels)
    names(value_labels)[1] <- "num"
    row.names(value_labels) <- NULL
    # Make the frequency table
    value_table <-
      data.frame(
        wtd.table(outcome_var[shown], na.show = TRUE,
                  weights = survey_weights[shown]),
        stringsAsFactors = FALSE
      )
    names(value_table)[1] <- "num"
    value_table$num <- as.numeric(as.character(value_table$num))
    # Deal with the skipped answers that are recorded as NA
    value_table$num[is.na(value_table$num)] <-
      value_labels$num[value_labels$labels %in% c("skipped", "Skipped")]
    value_table <-
      value_table %>% group_by(num) %>% dplyr::summarise(Freq = sum(Freq))
    # Merge the frequency table with the value labels
    value_table <-
      merge(x = value_table, y = value_labels, all.y = TRUE)
    # Set the ones that don't show up in frequency table as 0
    value_table$Freq[is.na(value_table$Freq)] <- 0
    # Prepare the table
    value_table$Prop <- value_table$Freq / sum(value_table$Freq)
    value_table$group <-
      ifelse(value_table$num %in% c(num_missing, num_DK),
             "DK/Missing",
             "Responses")
    value_table$group <-
      factor(value_table$group,
             levels = c("Responses",
                        "DK/Missing"))
    value_table$labels <- capitalize(value_table$labels)
    value_table$outcome <- outcome
    value_table$new_values <- new_values
    value_table$labels <- ifelse(
      value_table$group == "Responses",
      paste0(value_table$new_values, ". ",
             value_table$labels),
      value_table$labels
    )
    # Remove the ones not shown
    value_table <- value_table[!grepl(pattern = "not asked",
                                      x = value_table$labels,
                                      ignore.case = TRUE), ]
    # Get the summary statistics
    num_outcome <- as.numeric(outcome_var[shown])
    survey_weights <- survey_weights[shown]
    num_outcome <-
      relabel_var(
        old_var = num_outcome,
        old_labels = value_table$num,
        new_labels = value_table$new_values
      )
    num_outcome_missing <- is.na(num_outcome)
    num_outcome[is.na(num_outcome)] <- missing_recode
    # Get the percent missing
    percent_missing <-
      sum(num_outcome_missing) / length(num_outcome_missing)
    # Get the mean and se
    md <- if (percent_missing > 0.1) {
      # If more than 10 percent is missing, then we condition on normalized dummy variable for missingness
      summary(lm(num_outcome ~ scale(num_outcome_missing),
                 weights = survey_weights), robust = TRUE)$coefficients
    } else {
      summary(lm(num_outcome ~ 1, weights = survey_weights),
              robust = TRUE)$coefficients
    }
    # Put the summary statistics together
    value_sum <-
      data.frame(
        outcome = outcome,
        num = md[1, 1],
        se = md[1, 2],
        group = "Responses",
        sum_stat = paste0(roundfunc(md[1, 1]), " (", roundfunc(md[1, 2]), "); N=",
                          sum(shown)),
        N = sum(shown)
      )
    if (output_type == "num_outcome") {
      return(num_outcome)
    } else if (output_type == "value_table") {
      return(value_table)
    } else {
      return(value_sum)
    }
  }

# CDF fitting functions
gamma.dist.f <-
  function(x, shape, scale) {
    pgamma(x, shape = shape, scale = scale)
  }


fit <- function(x, ps, distribution, par_init) {
  err = function (data, par) {
    curve = distribution(data$x, par[1], par[2])
    sum((curve - data$y) ^ 2)
  }
  
  data = data.frame(x = x, y = ps)
  
  i = data.frame(pars = I(par_init)) %>%
    rowwise %>%
    mutate(error = err(data, pars)) %>%
    slice(which.min(error)) %>% .$pars
  
  initial_params = i[[1]]
  
  
  
  result = suppressWarnings(optim(par = initial_params,
                                  fn = err,
                                  data = data))
  data.frame(
    shape = result$par[1],
    scale = result$par[2],
    convergence = result$convergence,
    error = result$value
  )
}


fit.all <- function (data, distribution, par_init) {
  data %>%
    group_by(response.id) %>%
    do(fit(.$x, .$p, distribution, par_init = par_init)) %>%
    ungroup()
  
}

cum.dist <- function(params, x, distribution) {
  merge(x, params) %>%
    mutate(p = distribution(x, shape, scale))
}

gen.cum.dists <- function(df, x, dist, init) {
  df %>%
    fit.all(dist, par_init = init) %>%
    cum.dist(x, dist)
}

# Weighted summary statistics function
md_weight <- function(x, weights, which_stat) {
  md <- summary(lm(x ~ 1, weights = weights))$coefficients
  if (which_stat == "mean") {
    return(md[1, 1])
  } else {
    return(md[1, 2])
  }
}

# Regression analysis
regression_output <- function(dataset = d,
                              formula, variable_names, 
                              caption, output_model = FALSE,
                              kable_output = NULL) {
  md <- lm(
      as.formula(formula),
      data = dataset,
      weights = dataset$survey_weights
    )
  reg_sum <-
    summary(md,
    robust = TRUE)
  reg_table <- reg_sum$coefficients
  reg_stars <- rep("", nrow(reg_table))
  reg_stars[reg_table[, 4] < 0.05] <- "*"
  reg_stars[reg_table[, 4] < 0.01] <- "**"
  reg_stars[reg_table[, 4] < 0.001] <- "***"
  coef <-
    paste0(roundfunc(reg_table[, 1]),
           " (",
           roundfunc(reg_table[, 2]),
           ")",
           reg_stars)
  f_stat_p <- pf(reg_sum$fstatistic[1],
                 reg_sum$fstatistic[2],
                 reg_sum$fstatistic[3],
                 lower.tail = FALSE)
  f_stat <-
    paste0(
      "F(",
      reg_sum$fstatistic[2],
      ", ",
      reg_sum$fstatistic[3],
      ") = ",
      roundfunc(reg_sum$fstatistic[1]),
      "; p-value: ",
      ifelse(f_stat_p < 0.001, "<0.001", round(f_stat_p, 3))
    )
  # Generate the regression table
  if (output_model) {
    return(md)
  } else {
    output_sum <- data.frame(vars = c(variable_names, paste0("N = ", nrow(dataset))),
             coef = c(coef, f_stat))
    if (is.null(kable_output)) {
      return(kable(x = output_sum, format = "pandoc",
                   caption = caption, col.names = c("Variables", "Coefficients")))
    } else {
      return(kable(x = output_sum[kable_output,],
                   caption = caption, format = "pandoc",
                   col.names = c("Variables", "Coefficients"), row.names = FALSE))
    }
  }
}

# Load datasets
d0 <-
  read_sav("~/Google Drive/AI Public Opinion Surveys/Data/yougov/YALE0065_OUTPUT.sav")

# Sample with replacement to create 2000 responses
d <- as.data.frame(d0)
d$r_id <- 1:nrow(d)
# Generate fake sample weights
d$survey_weights <- d$weight

```

# Analysis of survey data

## General questions

The first set of questions in the survey helps us explain Americans' most basic understanding of AI and their general attitudes towards the technology. 

### Survey experiment: what the public considers AI, automation, machine learning, and robotics

#### Procedure

We use a survey experiment to understand what the public considers AI, automation, machine learning, and robotics. Each respondent is randomly assigned to consider one of these terms; he or she is asked:

>In your opinion, which of the following technologies, if any, uses [artificial intelligence (AI)/automation/machine learning/robotics]? Select all that apply.

Respondents consider the following technologies:

- Virtual assistants (e.g., Siri, Google Assistant, Amazon Alexa)
- Smart speakers (e.g., Amazon Echo, Google Home, Apple Homepod)
- Facebook photo tagging
- Google Search
- Recommendations for Netflix movies or Amazon ebooks
- Google Translate
- Driverless cars and trucks
- Social robots that can interact with humans
- Industrial robots used in manufacturing
- Drones that do not require a human controller

#### Results and discussion

The figure below shows the percent of respondents who selected each technology conditional on what term is randomly assigned to them. 

Americans exhibit nuanced perceptions of what they consider AI, automation, machine learning, and robotics. In general, respondents underestimate what prevalence of AI, machine learning, and robotics in everyday technological applications. For almost all technologies, respondents assigned the term "automation" are more likely to select the technology than those assigned to the other terms. 

Among those assigned the term "AI," a majority think that virtual assistants (63%), smart speakers (55%), driverless cars (56%), and autonomous drones use AI (54%). A majority of respondents mistakenly assume that Facebook photo tagging, Google Search, Netflix or Amazon recommendations, or Google Translate do not use AI or machine learning. Curiously, among those assigned the term "robotics," only 62% think social robots involve robotics, and only 70% think industrial robots involve robotics. [^2]

[^2]: One hypothesis to explain this result is that some of this is due to inattentiveness, where respondents did not select all items than they would if they gave it greater consideration.

```{r what_ai, echo=FALSE, fig.height=6, fig.keep='all', fig.width=7, cache=TRUE, warning=FALSE}
# Helper function 
whatsai <- function(technum, output_type = "num_outcome") {
  catvar_func(
  outcome = label(d0[,paste0("Q3new_", technum)]),
  outcome_var = d[,paste0("Q3new_", technum)],
  label_var = d0[,paste0("Q3new_", technum)],
  output_type = output_type,
  shown = rep(TRUE, nrow(d)),
  num_missing = 8,
  num_DK = -99, 
  new_values <- c(1, 0, NA, NA), 
  survey_weights = d$survey_weights, 
  missing_recode = wtd.mean(relabel_var(d[,paste0("Q3new_", technum)],
                                        c(1, 2, 8, 9), c(1, 2, NA, NA)), 
                            weights = d[,"survey_weights"], na.rm = TRUE)
  )  
}
# Run the analysis 
whatsai_d <- data.frame(techtreat = d$q3new_treat,
                        survey_weights = d$survey_weights,
                        do.call(cbind, lapply(1:10, whatsai)))
whatsai_d <- reshape2::melt(whatsai_d, id = c("techtreat", "survey_weights")) %>% 
  group_by(techtreat, variable) %>% dplyr::summarise(
    prop = md_weight(value, weights = survey_weights, which_stat = "mean"),
    se = md_weight(value, weights = survey_weights, which_stat = "se"),
    N = n())
whatsai_d$techtreat <- relabel_var(old_var = whatsai_d$techtreat, old_labels = c(1:4),
            new_labels = c("Artificial intelligence (AI)", 
                           "Automation", "Machine learning", "Robotics"))
whatsai_d$variable <- relabel_var(old_var = whatsai_d$variable, 
                                  old_labels = levels(whatsai_d$variable),
            new_labels = label(d0)[paste0("Q3new_", 1:10)]) %>% 
  factor(levels = rev(label(d0)[paste0("Q3new_", 1:10)]))
# Make the graph
ggplot(whatsai_d) +
  geom_point(aes(x = variable, y = prop, color = techtreat, shape = techtreat)) +
  coord_flip() +
  scale_y_continuous(labels = scales::percent, 
                     name = "Percent selected", limits = c(0, 1)) +
  scale_x_discrete(labels = function(x) str_wrap(x, width = 40), name = "Technologies") +
  scale_shape_discrete(name = "Terms randomly assigned to respondents") +
  scale_color_discrete(name = "Terms randomly assigned to respondents") +
  theme_bw() + theme(legend.position = "bottom", legend.direction = "vertical") +
  ggtitle("What the public considers AI, automation, machine\nlearning, and robotics")
```

### Support for developing AI

#### Procedure

Respondents are given the following definition of AI:

> Artificial Intelligence (AI) refers to computer systems that perform tasks or makes decisions that usually require human intelligence. AI can perform these tasks or make these decisions without explicit human instructions. Today, AI has been used in the following applications: [five randomly selected applications]

Each respondent views five applications randomly selected from a list of 14:

- Translate over 100 different languages
- Predict one’s Google searches
- Identify people from their photos
- Diagnose diseases like skin cancer and common illnesses
- Predict who are at risk of various diseases 
- Help run factories and warehouses
- Block spam email
- Play computer games
- Help conduct legal case research
- Categorize photos and videos
- Detect plagiarism in essays
- Spot abusive messages on social media 
- Predict what one is likely to buy online
- Predict what movies or TV shows one is likely to watch online 

Afterward, respondents are asked how much they support or oppose the development of AI. The responses are coded in the following way:

- -2 = Strongly oppose
- -1 = Somewhat oppose
- 0 = Neither support nor oppose
- 1 = Somewhat support
- 2 = Strongly support

#### Results and discussions

The distribution of responses is displayed in the figure below. The mean outcome and the corresponding standard error are also presented. 

Americans express mixed support for the development of AI, although more of them support than oppose the development of AI. A substantial minority (41%) somewhat or strongly support the development of AI. A smaller minority (22%) somewhat or strongly oppose the development of AI. Many express a neutral attitude: 28% state that they neither support nor oppose while 10% indicate they don't know. 

```{r support_dev_risks, echo=FALSE, fig.height=4, fig.keep='all', fig.width=7, warning=FALSE, cache=TRUE}
dev_ai_overall_mean <- wtd.mean(relabel_var(d$Q5, c(1:6, 8, 9), c(2:-2, NA, NA, NA)),
                            d$survey_weights, na.rm = TRUE)
# Frequency table
dev_ai_value_table <- catvar_func(
  outcome = label(d0$Q5),
  outcome_var = d$Q5,
  label_var = d0$Q5,
  output_type = "value_table",
  shown = rep(TRUE, nrow(d)),
  num_missing = 8,
  num_DK = 6,
  new_values <- c(2:-2, NA, NA, NA),
  survey_weights = d$survey_weights, 
  missing_recode = dev_ai_overall_mean
  )  
dev_ai_value_table$num <- c(2, 1, 0, -1, -2, 8, 9)
# Numerical values
dev_ai_value_sum <- catvar_func(
  outcome = label(d0$Q5),
  outcome_var = d$Q5,
  label_var = d0$Q5,
  output_type = "value_sum",
  shown = rep(TRUE, nrow(d)),
  num_missing = 8,
  num_DK = 6,
  new_values <- c(2, 1, 0, -1, -2, NA, NA, NA),
  survey_weights = d$survey_weights,
  missing_recode = dev_ai_overall_mean
  )  

# Make the chart
ggplot() +
  geom_bar(data = dev_ai_value_table, aes(x = num, y = Prop), stat = "identity",
           alpha = 0.55) +
  geom_text(data = dev_ai_value_table, aes(x = num, 
                                           label = paste0(roundfunc(Prop*100, 0), "%")), 
            y = 0.02) +
  scale_x_continuous(breaks = dev_ai_value_table$num[order(dev_ai_value_table$num)],
    labels = str_wrap(dev_ai_value_table$labels[order(dev_ai_value_table$num)], 
                      width = 15)) +
  facet_grid(~group, scales = "free_x", space = "free_x") + theme_bw() +
  geom_point(data = dev_ai_value_sum, aes(x = num), 
             y = max(dev_ai_value_table$Prop)+0.02) +
  geom_errorbarh(data = dev_ai_value_sum, aes(x = num, xmin = qnorm(0.025)*se + num,
                                       xmax = qnorm(0.975)*se + num,
                                       y = max(dev_ai_value_table$Prop)+0.02)) + 
  geom_text(data = dev_ai_value_sum, aes(x = num, label = sum_stat,
                                       y = max(dev_ai_value_table$Prop)+0.05)) +
  scale_y_continuous(labels = scales::percent, 
                     limits = c(0, max(dev_ai_value_table$Prop)+0.05)) +
  xlab("Outcomes") + ylab("Percentage of respondents") + 
  ggtitle("Support for developing AI")
```

#### Demographic predictors of support for developing AI

##### Procedure

We examine support for developing AI by the following demographic groups:

- Age group as defined by Pew Research Center: Millennial adults (born after 1980), Gen Xers (born 1965-1980), Baby Boomers (born 1946-1964), Silents/Greatest Generation (1945 and earlier)
- Gender: male, female
- Race: white, non-white
- Employment status: employed (full- or part-time), not employed
- Annual household income: less than \$30K annually, \$30-70K, \$70-100K, more than \$100K, prefer not to say
- Political party identification: Democrats, Republicans, Independents/Others
- Religion: Christian, follow other religions, non-religious
- Identifies as a born-again Christian: yes, no
- Completed a computer science undergraduate or graduate degree: yes, no
- Has computer science or programming experience: yes, no

We also use a multiple regression model to predict support for developing AI using the demographic variables listed above.

##### Results and discussion

The first figure below displays the average support for developing AI by demographic groups. Support for developing AI is negatively correlated with age. Males express greater support than females. Those who are employed express greater support than those who are not employed (i.e., retirees, students, or homemakers). Support is positively correlated with family income; support is the lowest among those who prefer not to report their income. Democrats express greater support for developing AI than either Republicans or Independents. Those who are Christian express less support compared to those who have other religious affiliation or are not religious; those who identify as "born-again" Christian exhibit less support than those who don't identify as such. Those with computer science (CS) or engineering degrees or those with CS and programming experience are more likely support developing AI than those who do not. 

The second figure displays the coefficient plot from a multiple regression model that includes all of the demographic variables. The outcome variable has been standardized, so it has mean 0 and unit variance. Significant predictors correlated with support for developing AI include:

- identifying as a Democrat (versus identifying as an Independent)
- having a family income of \$70-100K or greater than \$100K annually (versus having an income of less than \$30K annually)
- having CS or programming experience (versus not having such experience). 

Significant predictors correlated with opposition to developing AI include:

- being a Gen Xer or a Baby Bloomer (versus being a Millennial)
- being a female (versus being a male)
- identifying as a Christian (versus identifying as non-religious)

```{r demographic_support, echo=FALSE, fig.height=10.5, fig.keep='all', fig.width=7, cache=TRUE, warning=FALSE}
age_levels <- c("Age 18-33", "Age 34-49", "Age 50-68", "Age 69 and older")
d$demo_age <- relabel_var2(old_var = d$birthyr, old_labels = list(1980:2018, 1965:1980,
                                                     1946:1964, 1900:1945),
                           new_labels = age_levels)
d$demo_age <- factor(d$demo_age, levels = age_levels)
d$demo_gender <- ifelse(d$gender == 1, "Male", "Female")
d$demo_gender <- factor(d$demo_gender, c("Male", "Female"))
d$demo_white <- ifelse(d$race == 1, "White", "Non-white")
d$demo_white <- factor(d$demo_white, c("White", "Non-white"))
d$demo_employ <- ifelse(d$employ %in% c(1, 2), "Employed (full- or part-time)", 
                        "Not employed")
d$demo_employ <- factor(d$demo_employ, levels = c("Not employed", 
                                                  "Employed (full- or part-time)"))
d$demo_income <- relabel_var2(old_var = d$faminc_new, 
                              old_labels = list(1:3, 4:7, 8:9, 10:16, 97),
                           new_labels = c("Less than $30K", "$30-70K", "$70-100K",
                                          "More than $100K", "Prefer not to say income"))
d$demo_income <- factor(d$demo_income, levels = c("Less than $30K", "$30-70K", "$70-100K",
                                          "More than $100K", "Prefer not to say income"))
d$demo_pid3 <- relabel_var2(old_var = d$pid3, old_labels = list(1, 2, 3:5),
                           new_labels = c("Democrat", "Republican", "Independent/Other"))
d$demo_pid3 <- factor(d$demo_pid3, levels = c("Independent/Other", 
                                              "Democrat", "Republican"))
d$demo_rel <- ifelse(d$religpew %in% 1:4, "Christian", "Other religion")
d$demo_rel  <- factor(d$demo_rel, levels = c("No religious affiliation", "Christian",
                                             "Other religion"))
d$demo_rel[d$religpew %in% c(9, 10, 11)] <- "No religious affiliation"
d$demo_bornagain <- ifelse(d$pew_bornagain == 1, "Born-again", "Not born-again")
d$demo_bornagain <- factor(d$demo_bornagain, levels = c("Not born-again", "Born-again"))
d$demo_cs <- ifelse(d$Q4_2 == 1 | d$Q4_3 == 1,
                    "CS or engineering degree", "No CS or engineering degree")
d$demo_cs <- factor(d$demo_cs, levels = c("No CS or engineering degree", 
                                          "CS or engineering degree"))
d$demo_prog <- ifelse(d$Q4_1 == 1 | d$Q4_2 == 1 | d$Q4_3 == 1 | d$Q4_4 == 1,
                    "CS or programming experience", "No CS or programming experience")
d$demo_prog <- factor(d$demo_prog, levels = c("No CS or programming experience", "CS or programming experience"))
# Names of the demographic variables
demo_var <- names(d)[grep(pattern = "demo_", ignore.case = FALSE, x = names(d))]
# Clean up the data
d$Q5_clean <- relabel_var(old_var = d$Q5, old_labels = c(1:6, 8, 9),
                          new_labels = c(2, 1, 0, -1, -2, NA, NA, NA))
d$Q5_clean[is.na(d$Q5_clean)] <- dev_ai_overall_mean
d$Q5_clean <- scale(d$Q5_clean)

# Helper function 
demo_support <- function(demo, demo_value, outcome_var = d$Q5, label_var = d0$Q5,
                         demo_group, output_type = "value_sum") {
  temp <- catvar_func(
  outcome = label(label_var),
  outcome_var = outcome_var,
  label_var = label_var,
  output_type = output_type,
  shown = d[,demo] == demo_value,
  num_missing = 8,
  num_DK = 6,
  new_values <- c(2, 1, 0, -1, -2, NA, NA, NA),
  survey_weights = d$survey_weights,
  missing_recode = dev_ai_overall_mean
  )    
  return(data.frame(demo, demo_value, demo_group, temp))
}
# Function to aggregate
demo_support_values <- function(demo, demo_group, output_type = "value_sum") {
  levels_demo <- levels(d[,demo])
  lapply(levels_demo, demo_support, demo = demo, 
         demo_group = demo_group, output_type = output_type) %>% do.call(what = rbind)
}

# Make the summary statistics
d_value_sum_support <- rbind(
  demo_support_values(demo = "demo_age", demo_group = "Age groups"),
  demo_support_values(demo = "demo_gender", demo_group = "Gender"),
  demo_support_values(demo = "demo_white", demo_group = "Race"),
  demo_support_values(demo = "demo_employ", demo_group = "Employment status"),
  demo_support_values(demo = "demo_income", demo_group = "Income"),
  demo_support_values(demo = "demo_pid3", demo_group = "Political party identification"),
  demo_support_values(demo = "demo_rel", demo_group = "Religion"),
  demo_support_values(demo = "demo_bornagain", demo_group = "Born-again Christian"),
  demo_support_values(demo = "demo_cs", demo_group = "CS or engineering degree"),
  demo_support_values(demo = "demo_prog", demo_group = "CS or programming experience"))
  
# Generate graph
ggplot(data = d_value_sum_support, aes(x = demo_value, y = num, 
                                 ymin = qnorm(0.025) * se + num,
                                 ymax = qnorm(0.975) * se + num)) +
  #geom_hline(yintercept = 0, linetype = 2, alpha = 0.5) +
  geom_pointrange(position = position_dodge(width = 0.9)) + 
  geom_text(aes(label = sum_stat), nudge_y = 0.3, alpha = 0.6, size = 2.5) +
  coord_flip() + 
  scale_x_discrete(labels = function(x) str_wrap(x, width = 30),
    name = "Demographic characteristics (grouped by demographic variable)") + 
  scale_y_continuous(
    name = "Support for developing AI (-2 = Strongly oppose; 2 = Strongly support)") + 
   facet_wrap(~demo_group, scales = "free_y", ncol = 1) +
  ggtitle(str_wrap("Predicting support for developing AI using demographic characteristics: average support across groups", width = 60)) + 
  theme_bw()  

# Make the value frequency table
d_value_table_support <- rbind(
  demo_support_values(demo = "demo_age", demo_group = "Age groups",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_gender", demo_group = "Gender",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_white", demo_group = "Race",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_employ", demo_group = "Employment status",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_income", demo_group = "Income",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_pid3", demo_group = "Political party identification",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_rel", demo_group = "Religion",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_bornagain", demo_group = "Born-again Christian",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_cs", demo_group = "CS or engineering degree",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_cs", demo_group = "CS or programming experience",
                      output_type = "value_table"))

# Make second graph
# Make the graph
# d_value_table_support <-
#   d_value_table_support[!d_value_table_support$labels == "Skipped",]
# 
# ggplot(data = d_value_table_support, 
#        aes(x=demo_value, y=Prop, fill=labels)) +
#   geom_bar(aes(alpha = group), stat="identity", position = "fill") + 
#   scale_alpha_manual(values = c(0.6, 1), guide = "none") + xlab("Demographic groups") +
#   scale_y_continuous(name = "Percent of respondents", labels = scales::percent,
#                      limits = c(0, 1), expand = c(0, 0)) + coord_flip() + 
#   theme_bw() + theme(legend.position = "bottom") +
#   facet_grid(demo_group~., scales = "free_y", space = "free_y") +
#   ggtitle(str_wrap("Predicting support for developing AI using demographic characteristics: distribution of responses", width = 70))
```

```{r demographic_support2, echo=FALSE, fig.height=7, fig.keep='all', fig.width=7, cache=TRUE, warning=FALSE}
# Generate the data for the graph
dev_md <- lm(Q5_clean ~ demo_age + demo_gender + demo_white + demo_employ + 
    demo_pid3 + demo_income + demo_rel + demo_bornagain + demo_cs + 
    demo_prog, data = d, weights = d$survey_weights)
dev_md <- summary(dev_md, robust = TRUE)$coefficients %>% as.data.frame()
names(dev_md) <- c("num", "se", "t", "p")
dev_md$variables <- gsub(pattern = paste0(demo_var, collapse = "|"), replacement = "",
                         x = rownames(dev_md))
# Make the stars
dev_md$stars <- ""
dev_md$stars[dev_md$p < 0.05] <- "*"
dev_md$stars[dev_md$p < 0.01] <- "**"
dev_md$stars[dev_md$p < 0.001] <- "***"
# Make the text
dev_md$new_text <- paste0(roundfunc(dev_md$num), 
                      " (", roundfunc(dev_md$se), ")", dev_md$stars)
dev_md$variables <- factor(dev_md$variables, 
                           levels = rev(dev_md$variables[c(2:nrow(dev_md), 1)]))
# Make a graph
ggplot(data = dev_md, aes(x = variables, y = num, 
                                 ymin = qnorm(0.025) * se + num,
                                 ymax = qnorm(0.975) * se + num)) +
  geom_hline(yintercept = 0, linetype = 2, alpha = 0.5) +
  geom_pointrange(position = position_dodge(width = 0.9)) + 
  geom_text(aes(label = new_text), nudge_x = 0.3, alpha = 0.6) +
  coord_flip() + 
  scale_x_discrete(labels = function(x) str_wrap(x, width = 30),
                         name = "Demographic characteristics") + 
  scale_y_continuous(expand = c(0.1, 0.1), 
    name = "Coefficient estimates (outcome standardized)") + 
  ggtitle(str_wrap("Predicting support for developing AI using demographic characteristics: results from a multiple regression model that includes all demographic variables", width = 60)) + 
  theme_bw()
```

### Survey experiment: AI and/or robots should be carefully managed

#### Procedure

We perform a survey experiment that seeks to replicate a question from the 2018 Special Eurobarometer #460. The original question asked respondents how much they agree or disagree with the following statement:

>Robots and artificial intelligence are technologies that require careful management.

We asked a similar question except respondents are randomly assigned to consider one of these three statements:

1. AI and robots are technologies that require careful management.
2. AI is a technology that requires careful management.
3. Robots are technologies that require careful management. 

The responses are coded in the following way:

- -2 = Totally disagree
- -1 = Tend to disagree
- 1  = Tend to agree
- 2 = Totally agree

#### Results and discussion

The first figure below summarizes the results using all the data unconditional on treatment assignment. An overwhelming majority of Americans (82%) think that AI and/or robots should be carefully managed. Only 6% disagrees with the statement. 

The second figure summarizes the results conditional on treatment assignment. The regression table presents the treatment effect estimates of the question-wording. We find that variations in the question-wording produce minor, non-significant, differences in responses. 

```{r ai_managed, echo=FALSE, fig.height=4, fig.keep='all', fig.width=7, cache=TRUE, warning=FALSE, warning=FALSE}

# Overall mean for recoding missing values 
manage_ai_overall_mean <- wtd.mean(relabel_var(d$Q5b, c(1:5, 8, 9), 
                                              c(2, 1, -1, -2, NA, NA, NA)),
                            weights = d$survey_weights, na.rm = TRUE)
# Frequency table
manage_ai_value_table <- catvar_func(
  outcome = "Agreement that AI and robots should be carefully managed",
  outcome_var = d$Q5b,
  label_var = d0$Q5b,
  output_type = "value_table",
  shown = rep(TRUE, nrow(d)),
  num_missing = 8,
  num_DK = 5,
  new_values <- c(2, 1, -1, -2, NA, NA, NA),
  missing_recode = manage_ai_overall_mean, 
  survey_weights = d$survey_weights
  )  
manage_ai_value_table$num <- c(2, 1, -1, -2, 8, 9)
# Numerical values
manage_ai_value_sum <- catvar_func(
  outcome = label(d0$Q5b),
  outcome_var = d$Q5b,
  label_var = d0$Q5b,
  output_type = "value_sum",
  shown = rep(TRUE, nrow(d)),
  num_missing = 8,
  num_DK = 5,
  new_values <- c(2, 1, -1, -2, NA, NA, NA),
  missing_recode = manage_ai_overall_mean,
  survey_weights = d$survey_weights
  )  

# Make the chart
ggplot() +
  geom_bar(data = manage_ai_value_table, aes(x = num, y = Prop), stat = "identity",
           alpha = 0.55) +
  geom_text(data = manage_ai_value_table, aes(x = num, 
                                           label = paste0(roundfunc(Prop*100, 0), "%")), 
            y = 0.02) +
  scale_x_continuous(breaks = manage_ai_value_table$num[order(manage_ai_value_table$num)],
    labels = str_wrap(manage_ai_value_table$labels[order(manage_ai_value_table$num)], 
                      width = 15)) +
  facet_grid(~group, scales = "free_x", space = "free_x") + theme_bw() +
  geom_point(data = manage_ai_value_sum, aes(x = num), 
             y = max(manage_ai_value_table$Prop)+0.02) +
  geom_errorbarh(data = manage_ai_value_sum, aes(x = num, xmin = qnorm(0.025)*se + num,
                                       xmax = qnorm(0.975)*se + num,
                                       y = max(manage_ai_value_table$Prop)+0.02)) + 
  geom_text(data = manage_ai_value_sum, aes(x = num, label = sum_stat,
                                       y = max(manage_ai_value_table$Prop)+0.05)) +
  scale_y_continuous(labels = scales::percent, 
                     limits = c(0, max(manage_ai_value_table$Prop)+0.05)) +
  xlab("Outcomes") + ylab("Percentage of respondents") + 
  ggtitle(str_wrap("Agreement with statement that AI and/or robots should be carefully managed", width = 75))
```

```{r ai_managed_exp, echo=FALSE, fig.height=3, fig.keep='all', fig.width=7, cache=TRUE, warning=FALSE, warning=FALSE}
# Clean the data
d$Q5b_clean <- relabel_var(d$Q5b, c(1:5, 8, 9), c(2, 1, -1, -2, NA, NA, NA))
d$Q5b_missing <- is.na(d$Q5b_clean)
d$Q5b_clean[is.na(d$Q5b_clean)] <- wtd.mean(d$Q5b_clean[!is.na(d$Q5b_clean)], 
                                            weights = d$survey_weights[!is.na(d$Q5b_clean)])
d$q5b_treat_clean <- relabel_var(d$q5b_treat, c(1:3, 8, 9), 
                                 c("AI and robots", "AI", "Robots", NA, NA))

manage_func <- function(exp_group) {
  data.frame(catvar_func(
  outcome = label(d0$Q5b),
  outcome_var = d$Q5b,
  label_var = d0$Q5b,
  output_type = "value_sum",
  shown = d$q5b_treat_clean == exp_group,
  num_missing = 8,
  num_DK = 5,
  new_values <- c(2, 1, -1, -2, NA, NA, NA),
  missing_recode = manage_ai_overall_mean,
  survey_weights = d$survey_weights
  ), q5b_treat_clean = exp_group)
}
tech_manage <- lapply(c("AI and robots", "AI", "Robots"), manage_func) %>% 
  do.call(what = rbind)
tech_manage$q5b_treat_clean <- factor(tech_manage$q5b_treat_clean, 
                                      levels = c("AI", "Robots", "AI and robots"))
# Make the plot
ggplot(data = tech_manage, aes(x = q5b_treat_clean, y = num, 
                                 ymin = qnorm(0.025) * se + num,
                                 ymax = qnorm(0.975) * se + num)) +
  geom_pointrange(position = position_dodge(width = 0.9)) + 
  geom_text(aes(label = sum_stat), nudge_x = 0.4, alpha = 0.6) +
  coord_flip() + 
  scale_x_discrete(labels = function(x) str_wrap(x, width = 20),
                         name = "Experimental groups", expand = c(0.1, 0)) + 
  scale_y_continuous(expand = c(0.05, 0.05),
    name = "Agreement/disagreement with statement\n(-2 = Totally disagree; 2 = Totally agree)") + ggtitle(str_wrap("Agreement with statement that AI and/or robots should be carefully managed by experimental condition", width = 65)) + theme_bw()

# Attrition check
# Attrition rates by experimental groups
q5b_attrition <- d %>% group_by(q5b_treat_clean) %>% dplyr::summarise(
  attrition_prop = mean(Q5b_missing)*100,
  DK_prop = mean(Q5b == 5)*100
) 
q5b_attrition$missing_prop <- q5b_attrition$attrition_prop - q5b_attrition$DK_prop
kable(x = q5b_attrition, format = "pandoc",
      caption = "Survey experiment attrition check: agreement with statement that AI and/or robots should be carefully managed", col.names = c("Experimental condition", "Percent DK/missing", "Percent DK", "Percent missing"), digits = 2)
# Attrition check: regression results 
if (sum(q5b_attrition$attrition_prop > 0) > 0) {
  regression_output(formula = "Q5b_missing ~ q5b_treat_clean", 
                  variable_names = c("(Intercept)", "AI and robots", "Robots"),
                  caption = "Survey experiment attrition check: agreement with statement that AI and/or robots should be carefully managed")  
}
# Regression output 
if (sum(q5b_attrition$attrition_prop > 10) > 0) {
  d$q5b_treat_clean_ai_robots <- d$q5b_treat_clean == "AI"
  d$q5b_treat_clean_robots <- d$q5b_treat_clean == "Robots"
regression_output(formula = "Q5b_clean ~ q5b_treat_clean_ai_robots + q5b_treat_clean_robots + scale(Q5b_missing) + q5b_treat_clean_ai_robots:scale(Q5b_missing) + q5b_treat_clean_robots:scale(Q5b_missing)", 
                  variable_names = c("(Intercept)", "AI and robots", "Robots",
                                     "DK/missing", "AI and robots x DK/missing",
                                     "Robots x DK/missing"),
                  caption = "Survey experiment results: agreement with statement that AI and/or robots should be carefully managed (controlling for DK/missing responses)",
                  kable_output = c(1:3, 7))  
} else {
  regression_output(formula = "Q5b_clean ~ q5b_treat_clean",
                  variable_names = c("(Intercept)", "AI and robots", "Robots"),
                  caption = "Survey experiment results: agreement with statement that AI and/or robots should be carefully managed")
}

```

### Comparison with Eurobarometer data

The figure below summarizes responses to the managing AI and robots question from the 2017 Special Eurobarometer #460 by country. The question asked whether respondents agreed or disagree with the following statement:

>Robots and artificial intelligence are technologies that require careful management.

For the U.S., we use all the responses in our survey, unconditional on experimental condition given that the variation in question wording does not appear to affect responses. 

The percent of those in the U.S. who agree with the statement (82%) is not far off from the EU average (88% agree with the statement). Likewise, the percent of Americans who disagree with the statement (6% disagree) is comparable with the EU average (7% disagree). The U.S. ranks among the lowest in terms of the agreement with the statement in part due to the relatively high percentage who selected the "don't know" option. 

```{r eu, echo=FALSE, fig.height=5, fig.keep='all', fig.width=7, warning=FALSE, cache=TRUE}
# Get US data
us_ai_manage <- catvar_func(
    outcome = label(x = d0$Q5b),
    outcome_var = d$Q5b,
    label_var = d0$Q5b,
    output_type = "value_table",
    shown = rep(TRUE, nrow(d)),
    num_missing = 8,
    num_DK = 5, missing_recode = manage_ai_overall_mean,
    new_values <- c(2, 1, -1, -2, NA, NA, NA))

# Load Eurobarometer data
euro <- read.csv("~/Google Drive/AI Public Opinion Surveys/AnnualReview/data/EB/eb_2017_qd12_3.csv", stringsAsFactors=FALSE)
euro$group <- "No highlight"
euro$group[euro$Country == "EU"] <- "Highlight"
euro_com <- rbind(euro, data.frame(Country = "US", 
           Totally_agree = us_ai_manage$Prop[us_ai_manage$num == 1]*100,
           Tend_to_agree = us_ai_manage$Prop[us_ai_manage$num == 2]*100,
           Tend_to_disagree = us_ai_manage$Prop[us_ai_manage$num == 3]*100,
           Totally_disagree = us_ai_manage$Prop[us_ai_manage$num == 4]*100,
           Dont_know = us_ai_manage$Prop[us_ai_manage$num == 5]*100,
           Total_agree = sum(us_ai_manage$Prop[us_ai_manage$num %in% c(1, 2)])*100,
           Total_disagree = sum(us_ai_manage$Prop[us_ai_manage$num %in% c(3, 4)])*100,
           group = "Highlight")) %>% reshape2::melt(id = c("Country", "group"))
euro_responses <- c("Totally agree", "Tend to agree", "Tend to disagree",
                                   "Totally disagree", "Don't know", "Total agree",
                                   "Total disagree")
euro_com$variable <- relabel_var(euro_com$variable, old_labels = unique(euro_com$variable),
                                 new_labels = euro_responses)
euro_com$variable <- factor(euro_com$variable, euro_responses[c(5, 4, 3, 2, 1, 6, 7)])
euro_com$stacked <- !euro_com$variable %in% c("Total agree", "Total disagree")
euro_com$Country <- factor(euro_com$Country,
                           levels = euro_com$Country[euro_com$variable == "Total agree"][order(euro_com$value[euro_com$variable == "Total agree"])])
# Make the graph
ggplot(data = euro_com[euro_com$stacked,], aes(x=Country, y=value/100, fill=variable)) +
  geom_bar(aes(alpha = group), stat="identity") + 
  scale_alpha_manual(values = c(1, 0.6), guide = "none") +
  scale_fill_manual(values = c("grey65", "darkred", "red", "cornflowerblue", "darkblue"),
                    name = "Responses") + xlab("Countries") +
  scale_y_continuous(name = "Percent of respondents", labels = scales::percent,
                     limits = c(0, 1), expand = c(0, 0)) +
  theme_bw() + theme(legend.position = "bottom") +
  guides(fill = guide_legend(reverse = TRUE)) +
  ggtitle(str_wrap("Agreement with statement that robots and AI require careful management (EU data from 2017 Special Eurobarometer #460)", width = 70))

```

## AI governance challenges

### Prioritizing governance challenges

#### Procedure

This set of questions seeks to understand how Americans prioritize policy issues associated with AI. Respondents are asked to consider five AI governance challenges, randomly selected from [13 potential ones](#gov_challenges). A short paragraph explains each AI governance challenge. The 13 challenges include:

- Bias in AI used in jobs hiring
- Bias in AI used in the criminal justice system
- Accuracy in diagnosing diseases
- Data privacy 
- Digital manipulation
- AI-enhanced cyberattacks
- AI-enhanced surveillance
- U.S.-China arms race
- Value alignment
- Autonomous weapons
- Mass technical unemployment
- Failure of critical AI systems

After considering each governance challenge, respondents are asked how likely they think the challenge will affect large numbers of people 1) in the U.S. and 2) around the world within ten years. Respondents indicate the likelihood using the following multiple choices:

- Very unlikely: less than 5% chance
- Unlikely: 5-20% chance
- Somewhat unlikely: 20-40% chance
- Equally likely as unlikely: 40-60% chance
- Somewhat likely: 60-80% chance
- Likely: 80-95% chance 
- Very likely: more than 95% chance

For the analysis, the multiple-choice answers are converted to the median value of each answer choice range. For instance, the "very unlikely" answer is converted to 2.5%. 

Finally, respondents are asked how important it is for tech companies and governments to manage each governance challenge carefully. They are given the following answer options:

- 3 = Very important
- 2 = Somewhat important
- 1 = Not too important
- 0 = Not at all important

#### Results and discussion 

The results from the AI governance challenge questions are plotted below. In the first figure, the x-axis is the perceived likelihood of the problem happening to large numbers of people in the U.S. In the second figure, the x-axis is the perceived likelihood of the problem happening to large numbers of people around the world. The y-axes of the two plots are the same. A dot represents the mean perceived likelihood and issue importance, and the correspondent ellipse contains the 95% confidence region. 

Americans consider all the AI governance challenges to be important: the mean perceived issues importance of each governance challenge is between "somewhat important" (2) and "very important" (3). While the variation in perceived issue importance is small, there is greater variation in Americans' perceived likelihood of each governance challenge impacting a large number of people in the next 10 years -- ranging from 55% likelihood for lethal autonomous weapons to 69% likelihood for AI-enhanced surveillance in the U.S. 

The AI governance challenges Americans think are most likely to impact large numbers of people and important for tech companies and governments to tackle are found in the upper-right quadrant of the two plots. These issues include protecting data privacy, AI-enhanced cyberattacks, AI-enhanced surveillance, and digital manipulation. These issues have been widely reported in the media in recent months. Interestingly, Americans also prioritize AI governance challenges that are not commonly discussed or may appear highly complex, such as the U.S.-China arms race or value alignment. Nevertheless, they do not consider the failure of critical AI systems to be likely or very important as a policy issue. 

In the third figure, we compare respondents' perceived likelihood of each governance challenge impacting people in the U.S. versus around the world. The results are largely similar for most governance challenges except for two. Americans think that value alignment and lethal autonomous weapons are more likely affect large numbers of people around the world than in the U.S. within the next decade.  

```{r ai_risks, echo=FALSE, fig.height=7, fig.keep='all', fig.width=7, cache=TRUE, warning=FALSE, warning=FALSE}
# Clean up data
# Labels for the government challenges
ai_gov <- c("Hiring bias", "Criminal justice bias", "Disease diagnosis",
                  "Data privacy", "Autonomous vehicles", "Digital manipulation",
                  "Cyberattacks", "Surveillance", "U.S.-China arms race",
                  "Value alignment", "Autonomous weapons", "Mass unemployment", 
                  "Critical AI system failure")
# Median values for the answer choice ranges
mc_p_med <- c(median(0:5), median(5:20), median(20:40), median(40:60),
              median(60:80), median(80:95), median(95:100))

var_name = "Q8_"
risk_num = 1
foo1 <- d[,paste0(var_name, risk_num)]
# Helper function 
ai_gov_clean <- function(risk_num, var_name) {
  # Convert multiple-choice outcomes to slider outcomes 
  mc_outcome <- relabel_var(d[,paste0(var_name, risk_num)], 
                            old_labels = c(1:8, 98, 99), 
                            new_labels = c(mc_p_med, NA, NA, NA))
  # Clean the importance outcomes
  importance <- d[,paste0("Q10_", risk_num)] 
  importance <- relabel_var(importance, old_labels = c(1:5, 8, 9), 
                        new_labels = c(3, 2, 1, 0, NA, NA, NA))
  # Make into a dataframe
  temp <- data.frame(respondent_id = d$r_id,
               gov_challenge = ai_gov[risk_num],
                    prob = mc_outcome,
                    importance = importance,
               survey_weights = d$survey_weights,
               demo_age = d$demo_age,
               demo_gender = d$demo_gender,
               demo_white = d$demo_white,
               demo_employ = d$demo_employ,
               demo_income = d$demo_income,
               demo_pid3 = d$demo_pid3,
               demo_rel = d$demo_rel,
               demo_bornagain = d$demo_bornagain,
               demo_cs = d$demo_cs,
               demo_prog = d$demo_prog
               )
  # Remove the respondents who aren't show the risk
  return(temp[!is.na(d[,paste0("Q8_challenge_", risk_num)]) &
                       d[,paste0("Q8_challenge_", risk_num)] == 1,])
}

# Clean up the data
# US
ag_clean_US <- do.call(rbind, lapply(1:13, ai_gov_clean, var_name = "Q8_"))
# Recode the missing values
ag_clean_US$prob_missing <- is.na(ag_clean_US$prob)
ag_clean_US$importance_missing <- is.na(ag_clean_US$importance)
ag_clean_US$prob[is.na(ag_clean_US$prob)] <- wtd.mean(ag_clean_US$prob, 
                                                      weights = ag_clean_US$survey_weights,
                                                      na.rm =  TRUE)
ag_clean_US$importance[is.na(ag_clean_US$importance)] <- wtd.mean(ag_clean_US$importance, 
                                                      weights = ag_clean_US$survey_weights,
                                                      na.rm =  TRUE)
ag_clean_US$geo <- "U.S."

# World 
ag_clean_world <- do.call(rbind, lapply(1:13, ai_gov_clean, var_name = "Q9_"))
# Recode the missing values
ag_clean_world$prob_missing <- is.na(ag_clean_world$prob)
ag_clean_world$importance_missing <- is.na(ag_clean_world$importance)
ag_clean_world$prob[is.na(ag_clean_world$prob)] <- 
  wtd.mean(ag_clean_world$prob, weights = ag_clean_world$survey_weights,
                                                      na.rm =  TRUE)
ag_clean_world$importance[is.na(ag_clean_world$importance)] <- 
  wtd.mean(ag_clean_world$importance, weights = ag_clean_world$survey_weights,
                                                      na.rm =  TRUE)
ag_clean_world$geo <- "World"
# Check the percent of missing data
ag_clean_all <- rbind(ag_clean_US, ag_clean_world)
ag_missing <- ag_clean_all %>% group_by(geo, gov_challenge) %>% dplyr::summarise(
  prob_missing = mean(prob_missing),
  importance_missing = mean(importance_missing)
) %>% as.data.frame()

# Summarize the data
# Helper function
ag_sum_func <- function(ag, geo) {
  survey_weights <- ag_clean_all$survey_weights[ag_clean_all$gov_challenge == ag &
                                                   ag_clean_all$geo == geo]
  # Likelihood 
  md_prob <- if (ag_missing$prob_missing[ag_missing$gov_challenge == ag &
                                         ag_missing$geo == geo] > 0.1) {
    # If more than 10 percent is missing, then we condition on normalized dummy variable for missingness
      summary(lm(prob ~ scale(prob_missing), 
                 data = ag_clean_all[ag_clean_all$gov_challenge == ag &
                                   ag_clean_all$geo == geo,],
               weights = survey_weights), 
              robust = TRUE)$coefficients
    } else {
      summary(lm(prob ~ 1, 
                 data = ag_clean_all[ag_clean_all$gov_challenge == ag &
                                   ag_clean_all$geo == geo,],
               weights = survey_weights), 
              robust = TRUE)$coefficients
    }
  # Issue importance
  md_importance <- if (ag_missing$importance_missing[ag_missing$gov_challenge == ag &
                                         ag_missing$geo == geo] > 0.1) {
    # If more than 10 percent is missing, then we condition on normalized dummy variable for missingness
      summary(lm(importance ~ scale(importance_missing), 
                 data = ag_clean_all[ag_clean_all$gov_challenge == ag &
                                   ag_clean_all$geo == geo,],
               weights = survey_weights), 
              robust = TRUE)$coefficients
    } else {
      summary(lm(importance ~ 1, 
                 data = ag_clean_all[ag_clean_all$gov_challenge == ag &
                                   ag_clean_all$geo == geo,],
               weights = survey_weights), 
              robust = TRUE)$coefficients
    }
  data.frame(gov_challenge = ag, prob = md_prob[1,1], importance = md_importance[1,1],
             prob_se = md_prob[1,2], importance_se = md_importance[1,2],
             N = length(survey_weights),
             region = geo)
}
# Run the analysis
ag_sum_US <- lapply(ai_gov, ag_sum_func, geo = "U.S.") %>% do.call(what = rbind)
ag_sum_world <- lapply(ai_gov, ag_sum_func, geo = "World") %>% do.call(what = rbind)
# Combine the U.S. and world results
ag_sum_all <- rbind(ag_sum_US, ag_sum_world)
# Function to get the confidence ellipses
ag_cr_func <- function(ag, geo, alpha = 0.5, m = 2000) {
   # Likelihood 
  survey_weights <- ag_clean_all$survey_weights[ag_clean_all$gov_challenge == ag &
                                                   ag_clean_all$geo == geo]
  md_prob <- if (ag_missing$prob_missing[ag_missing$gov_challenge == ag &
                                         ag_missing$geo == geo] > 0.1) {
    # If more than 10 percent is missing, then we condition on normalized dummy variable for missingness
      lm(prob ~ scale(prob_missing), 
                 data = ag_clean_all[ag_clean_all$gov_challenge == ag &
                                   ag_clean_all$geo == geo,],
         weights = survey_weights)
    } else {
      lm(prob ~ 1, 
                 data = ag_clean_all[ag_clean_all$gov_challenge == ag &
                                   ag_clean_all$geo == geo,],
         weights = survey_weights)
    }
  # Issue importance
  md_importance <- if (ag_missing$importance_missing[ag_missing$gov_challenge == ag &
                                         ag_missing$geo == geo] > 0.1) {
    # If more than 10 percent is missing, then we condition on normalized dummy variable for missingness
      lm(importance ~ scale(importance_missing), 
                 data = ag_clean_all[ag_clean_all$gov_challenge == ag &
                                   ag_clean_all$geo == geo,],
               weights = survey_weights)
    } else {
      lm(importance ~ 1, 
                 data = ag_clean_all[ag_clean_all$gov_challenge == ag &
                                   ag_clean_all$geo == geo,],
               weights = survey_weights)
    }
  # Generate the variance-covariance matrix using the residuals from our models
  cov_res <- cov(cbind(md_prob$residuals, md_importance$residuals))
  # Generate data for the confidence ellipse
  bivCI_res <- bivCI(s = cov_res, xbar = c(md_prob$coefficients[1],
                                               md_importance$coefficients[1]),
      n = length(survey_weights), alpha = alpha, m = m)
  return(data.frame(gov_challenge = ag, prob = bivCI_res$x, importance = bivCI_res$y))
}
ag_cr_US <- do.call(rbind, lapply(ai_gov, ag_cr_func, geo = "U.S."))
ag_cr_US$region <- "U.S."
ag_cr_world <- do.call(rbind, lapply(ai_gov, ag_cr_func, geo = "World"))
ag_cr_world$region <- "World"
ag_cr_all <- rbind(ag_cr_US, ag_cr_world)
```

```{r ai_risks_2, echo=FALSE, fig.height=4, fig.keep='all', fig.width=7, cache=TRUE, warning=FALSE, warning=FALSE}
# Plot the US data
ggplot() + 
  geom_path(data = ag_cr_US, aes(x = prob/100, y = importance, color = gov_challenge),
            alpha = 0.3) + 
  geom_point(data = ag_sum_US, aes(x = prob/100, y = importance, 
                                color = gov_challenge), size = 3) +
  geom_text_repel(data = ag_sum_US, aes(x = prob/100, y = importance, 
                                     label = str_wrap(gov_challenge, width = 20)),
                  point.padding = 0.75, segment.alpha = 0.6) +
  scale_x_continuous(name = "Likelihood of impacting large numbers of people in the US within 10 years", labels = scales::percent, limits = c(0.5, 0.725)) + 
  ylab("Issue importance\n(0 = Not at all important; 3 = Very important)") +
  theme_bw() + theme(legend.position="none") + 
  ggtitle("Perceptions of AI governance challenges in the U.S.")

# Plot the world data
ggplot() + 
  geom_path(data = ag_cr_world, aes(x = prob/100, y = importance, color = gov_challenge),
            alpha = 0.3) + 
  geom_point(data = ag_sum_world, aes(x = prob/100, y = importance, 
                                color = gov_challenge), size = 3) +
  geom_text_repel(data = ag_sum_world, aes(x = prob/100, y = importance, 
                                     label = str_wrap(gov_challenge, width = 20)),
                  point.padding = 0.75, segment.alpha = 0.6) +
  scale_x_continuous(name = "Likelihood of impacting large numbers of people around the world within 10 years", labels = scales::percent, limits = c(0.5, 0.725)) + 
  ylab("Issue importance\n(0 = Not at all important; 3 = Very important)") +
  theme_bw() + theme(legend.position="none") +
  ggtitle("Perceptions of AI governance challenges around the world")
```

```{r ai_risks_3, echo=FALSE, fig.height=7, fig.keep='all', fig.width=7, cache=TRUE, warning=FALSE, warning=FALSE}
# Comparing likelihood of things happening
ag_sum_all$region <- factor(ag_sum_all$region, levels = c("World", "U.S."))
ggplot(data = ag_sum_all, aes(x = region, y = prob/100, 
                                 ymin = (qnorm(0.025) * prob_se + prob)/100,
                                 ymax = (qnorm(0.975) * prob_se + prob)/100)) +
  geom_pointrange(position = position_dodge(width = 0.9)) + 
  coord_flip() + 
  scale_x_discrete(labels = function(x) str_wrap(x, width = 30),
    name = NULL) + 
  scale_y_continuous(name = "Likelihood of impacting large numbers of people within 10 years", labels = scales::percent) +
  theme_bw() + 
  facet_wrap(~gov_challenge, ncol = 3) +
  theme(legend.position="bottom") + 
  ggtitle("Comparing perceived likelihood: in U.S. vs. around the world")
```

### Demographic predictors of issue importance

In the figure below, we summarize perceptions of issue importance by demographic subgroup and AI governance challenge. In the first graph below, we show the percent of respondents within each demographic subgroup who considers each governance challenge a "very important" issue. For the second graph, to make the differences appear more visible, we subtract the overall mean across all responses from each subgroup-governance challenge mean. 

Two highly visible patterns emerge from the figures. First, age is positively correlated with perceived issue importance across all AI governance challenges. In a previous question, we find that older Americans, compared with younger Americans, are less enthusiastic about developing AI. Our nuanced questions about AI governance challenges might explain this lack of enthusiasm: older respondents express greater concern about each of the governance issues when compared with their younger counterparts. 

Second, those with CS or engineering degrees -- compare with those who do not -- rate all AI governance challenges as less important. This result could explain our previous finding that those with CS or engineering degrees tend to exhibit greater support for developing AI. 

```{r ai_risks_demo, echo=FALSE, fig.height=10, fig.keep='all', fig.width=7, cache=TRUE, warning=FALSE, warning=FALSE}
# Helper function to clean up the data 
heatmap_func <- function(dem_var, dem_group) {
  temp <- ag_clean_US %>% group_by_(dem_var, 'gov_challenge') %>% dplyr::summarise(
  p_importance = wtd.mean(importance >= 3, weights = survey_weights),
  importance = wtd.mean(importance, weights = survey_weights),
  group = dem_group
)
  names(temp)[1] <- "characteristic"
  return(as.data.frame(temp))
}

# Summarize data by demographic subgroup and AI governance challenge 
heat_map_res <- rbind(heatmap_func(dem_var = "demo_age", dem_group = "Age groups"),
heatmap_func(dem_var = "demo_gender", dem_group = "Gender"),
heatmap_func(dem_var = "demo_white", dem_group = "Race"),
heatmap_func(dem_var = "demo_employ", dem_group = "Employment status"),
heatmap_func(dem_var = "demo_pid3", dem_group = "Political party identification"),
heatmap_func(dem_var = "demo_income", dem_group = "Household income"),
heatmap_func(dem_var = "demo_rel", dem_group = "Religion"),
heatmap_func(dem_var = "demo_bornagain", dem_group = "Born-again Christian"),
heatmap_func(dem_var = "demo_cs", dem_group = "CS or engineering degree"),
heatmap_func(dem_var = "demo_prog", dem_group = "CS or programming experience"))
# Clean up the data
heat_map_res$characteristic <- factor(heat_map_res$characteristic, 
                                      levels = rev(levels(heat_map_res$characteristic)))
heat_map_res$gov_challenge <- factor(heat_map_res$gov_challenge,
                                     levels = ag_sum_US$gov_challenge[order(ag_sum_US$importance)])
# Mean center the outcome
heat_map_res$importance_mc <- heat_map_res$importance - 
  wtd.mean(ag_clean_US$importance, weights = ag_clean_US$survey_weights)

# Make first graph
ggplot(heat_map_res, aes(x = gov_challenge, y = characteristic, fill = p_importance))+
  geom_bin2d(color = "white") + xlab("AI governance challenges") + 
  scale_y_discrete(name = "Demographic subgroups",
                   labels = function(x) str_wrap(x, width = 30)) +
  geom_text(aes(x = gov_challenge, y = characteristic, 
                label = roundfunc(p_importance*100, 0)), color = "black", size = 3) + 
  scale_fill_gradient(name = "Percent who considers the issue very important",
                      low = "#f7fbff", high = "#08519c", labels = scales::percent) + 
    theme_bw() + theme(legend.position = "bottom",
                       axis.text.x = element_text(angle = 90, hjust = 1)) +
  guides(fill = guide_colourbar(title.position="top", title.hjust = 0.5,
                                barwidth = 15)) +
  ggtitle(str_wrap("AI governance challenges: issue importance by U.S. demographic subgroups", width = 60))
```

```{r ai_risks_demo2, echo=FALSE, fig.height=10, fig.keep='all', fig.width=7, cache=TRUE, warning=FALSE, warning=FALSE}
# Make second graph
ggplot(heat_map_res, aes(x = gov_challenge, y = characteristic, fill = importance_mc))+
  geom_bin2d(color = "white") + xlab("AI governance challenges") + 
  scale_y_discrete(name = "Demographic subgroups",
                   labels = function(x) str_wrap(x, width = 30)) +
  geom_text(aes(x = gov_challenge, y = characteristic, 
                label = roundfunc(importance_mc, 1)), color = "black", size = 3) + 
  scale_fill_gradient2(name = "Mean-centered issue importance\n(Smaller value = less important; Greater value = more important)", 
                       low = "#1b7837", mid = "#f7f7f7", high = "#762a83") + 
    theme_bw() + theme(legend.position = "bottom", 
                       axis.text.x = element_text(angle = 90, hjust = 1)) +
  guides(fill = guide_colourbar(title.position="top", title.hjust = 0.5, 
                                barwidth = 15)) +
  ggtitle(str_wrap("AI governance challenges: issue importance by U.S. demographic subgroups", width = 60))
```

## Trust of actors to develop and manage AI

### Procedure 
Respondents are asked how much confidence they have in various actors to develop AI. They are randomly assigned five actors out of 15 to evaluate. For actors that are not well-known to the public (e.g., NATO, CERN, OpenAI, etc.), a short description is provided. 

Confidence is measured using a four-point scale:

- 0 = No confidence at all
- 1 = Not too much confidence
- 2 = A fair amount of confidence
- 3 = A great deal of confidence 

In addition, respondents are asked how much confidence, if any, they have in various actors to manage the development and use of AI in the best interests of the public. They are randomly assigned five out of 15 actors to evaluate. For actors that are not well-known to the public (e.g., AAAI, Partnership on AI, etc.), a [short description](#actors_appendix) is provided. 

Confidence is measured using the same four-point scale used in the trust to develop AI question described above.

### Results and discussion 

The figure below compares respondents' evaluations of the actors. We sort the actors are into four categories: U.S. government, international, corporate, and other.

In general, Americans do not have a great deal of confidence in any of the actors to develop AI in the best interest of the public. For all actors, the average rating is between "not too much confidence" (1) and "a fair amount of confidence" (2). The U.S. military and university researchers are the most trusted groups to develop AI. Intergovernmental research organizations and non-profit AI research organizations (e.g., OpenAI) have also received relatively high ratings. In general, respondents place greater confidence in tech companies than in civilian U.S. government agencies or NATO. But there is one notable exception: the public rates Facebook is as the least trustworthy of all the actors. Our survey was conducted three months after the fallout of the Facebook/Cambridge Analytica scandal; therefore, respondents may have low levels of general trust in Facebook. 

The results on the public's trust of various actors to manage the develop and use of AI provided are similar to the results discussed above. Once again, Americans do not have a great deal of confidence in any of the actors. The public places the most trust in Partnership on AI and non-governmental scientific organizations (e.g., AAAI) to manage AI. Trust in the U.S. government to manage AI is somewhat lower than tech companies and international governmental organizations. Again, Facebook stood out as an outlier: respondents give it a much lower rating than any other group.  

```{r trust, echo=FALSE, fig.height=10, fig.keep='all', fig.width=7, cache=TRUE, warning=FALSE}

# Labels for the actors
dev_actors <- c("U.S. military", "U.S. civilian government",
                "NSA", "FBI", "CIA", "NATO", 
                "Intergovernmental research organizations (e.g., CERN)",
                "Tech companies", "Google", "Facebook",
                "Apple", "Microsoft", "Amazon", "Non-profit (e.g., OpenAI)", 
                "University researchers")
# Overall mean
ai_dev_overall_mean <- wtd.mean(relabel_var(as.numeric(unlist(d[,paste0("Q6_", 1:15)])), 
            c(1:5, 8, 9), c(3, 2, 1, 0, NA, NA, NA)), 
         weights = rep(d$survey_weights, 15))
# Helper function
ai_dev_func <- function(variable_number, output_type) {
  catvar_func(outcome = dev_actors[variable_number], 
              outcome_var = d[,paste0("Q6_", variable_number)], 
              shown = d0[,paste0("Q6_org_", variable_number)] == 1,
              label_var = d0[,paste0("Q6_", variable_number)], 
              num_missing = 8, num_DK = 5,
              new_values = c(3, 2, 1, 0, NA, NA, NA),
              survey_weights = d$survey_weights,
              missing_recode = ai_dev_overall_mean,
              output_type)
}

# Frequency table
trust_dev_table <- do.call(rbind, lapply(X = 1:15, ai_dev_func, 
                                   output_type = "value_table"))
# Summary statistics
trust_dev_res <- do.call(rbind, lapply(X = 1:15, ai_dev_func, 
                                   output_type = "value_sum"))

# Classify the actors
classify_org <- data.frame(outcome = unique(trust_dev_res$outcome),
                           Org = c(rep("U.S. government", 5),
                                   rep("International", 2),
                                   rep("Corporate", 6),
                                   rep("Other", 2)))
trust_dev_res <- merge(x = trust_dev_res, y = classify_org, all.x = TRUE)
trust_dev_res$Org <- factor(trust_dev_res$Org, 
                            levels = c("U.S. government", "International",
                                           "Corporate", "Other"))
trust_dev_res$group <- "Trust in various actors to develop AI in the interest of the public"

# Labels for the actors
manage_actors <- c("U.S. federal government", "U.S. state governments", 
                "International organizations", "UN", 
                "Intergovernmental research organizations (e.g., CERN)", 
                "Tech companies", "Google",
                "Facebook", "Apple", "Microsoft", "Amazon", 
                "Non-government scientific organization (e.g., AAAI)", "Partnership on AI")
# Overall mean
ai_manage_overall_mean <- wtd.mean(relabel_var(as.numeric(unlist(d[,paste0("Q7_", 1:13)])), 
            c(1:5, 8, 9), c(3, 2, 1, 0, NA, NA, NA)), 
         weights = rep(d$survey_weights, 13))
# Helper function
ai_manage_func <- function(variable_number, output_type) {
  catvar_func(outcome = manage_actors[variable_number], 
              outcome_var = d[,paste0("Q7_", variable_number)], 
              shown = d[,paste0("Q7_org_", variable_number)] == 1,
              label_var = d0[,paste0("Q7_", variable_number)], 
              num_missing = 8, num_DK = 5,
              new_values = c(3, 2, 1, 0, NA, NA, NA), 
              missing_recode = ai_manage_overall_mean,
              output_type)
}

# Frequency table
trust_manage_table <- do.call(rbind, lapply(X = 1:13, ai_manage_func, 
                                   output_type = "value_table"))
# Summary statistics
trust_manage_res <- do.call(rbind, lapply(X = 1:13, ai_manage_func, 
                                   output_type = "value_sum"))

# Classify the actors
classify_org <- data.frame(outcome = unique(trust_manage_res$outcome),
                           Org = c(rep("U.S. government", 2),
                                   rep("International", 3),
                                   rep("Corporate", 6),
                                   rep("Other", 2)))
trust_manage_res <- merge(x = trust_manage_res, y = classify_org, all.x = TRUE)
trust_manage_res$Org <- factor(trust_manage_res$Org, 
                            levels = rev(c("Other", "Corporate", 
                                       "International", "U.S. government")))
trust_manage_res$group <- "Trust in various actors to manage AI in the interest of the public"
trust_res <- rbind(trust_dev_res, trust_manage_res)

# Plot the graph
trust_res$outcome <- factor(trust_res$outcome, levels = rev(levels(trust_res$outcome)))
ggplot(data = trust_res, aes(x = outcome, y = num, 
                                 ymin = qnorm(0.025) * se + num,
                                 ymax = qnorm(0.975) * se + num)) +
  geom_pointrange(aes(color = group, shape = group), 
                  position = position_dodge(width = 1)) + 
  coord_flip() + 
  scale_x_discrete(labels = function(x) str_wrap(x, width = 30),
                         name = "Institutions") + 
  scale_y_continuous(expand = c(0.1, 0.1),
    name = "Perceived trust (0 = No confidence at all; 3 = A great deal of confidence)") + theme_bw() + 
  facet_grid(Org~., scales = "free_y", space = "free_y", 
             labeller = label_wrap_gen(width = 35)) +
  theme(legend.position="bottom", legend.direction = "vertical") +
  scale_color_manual(values = c("cornflowerblue", "coral"), name = "Outcome measure") +
  scale_shape_discrete(name = "Outcome measure") + 
  ggtitle(str_wrap("Trust in various actors to develop and manage AI in the interest of the public", width = 60))

# Make a table
# Clean the data
trust_dev_res_clean <- trust_dev_res[,c("outcome", "sum_stat", "Org")]
trust_dev_res_clean$outcome <- as.character(trust_dev_res_clean$outcome)
trust_dev_res_clean$Org <- as.character(trust_dev_res_clean$Org)
names(trust_dev_res_clean)[names(trust_dev_res_clean) == "sum_stat"] <- "dev_ai"
trust_manage_res_clean <- trust_manage_res[,c("outcome", "sum_stat", "Org")]
trust_manage_res_clean$outcome <- as.character(trust_manage_res_clean$outcome)
trust_manage_res_clean$Org <- as.character(trust_manage_res_clean$Org)
names(trust_manage_res_clean)[names(trust_manage_res_clean) == "sum_stat"] <- "manage_ai"
# Marge the data
trust_table <- merge(x = trust_dev_res_clean, y = trust_manage_res_clean, 
                     all = TRUE, by = "outcome")
trust_table$Org <- ifelse(is.na(trust_table$Org.x), trust_table$Org.y, trust_table$Org.x)
trust_table <- trust_table[,c("outcome", "Org", "dev_ai", "manage_ai")]
trust_table$dev_ai <- as.character(trust_table$dev_ai)
trust_table$manage_ai <- as.character(trust_table$manage_ai)

trust_table$dev_ai[is.na(trust_table$dev_ai)] <- ""
trust_table$manage_ai[is.na(trust_table$manage_ai)] <- ""
trust_table$Org <- factor(trust_table$Org, 
                          levels = c("U.S. government", "International", 
                                     "Corporate", "Other"))
trust_table$outcome <- factor(trust_table$outcome, levels = rev(levels(trust_res$outcome)))
trust_table <- trust_table[order(trust_table$Org, trust_table$outcome),]
# Output the table via kable
kable(trust_table, caption = "Trust in various actors to develop and manage AI in the interest of the public", 
      row.names = FALSE, format = "pandoc", 
      col.names = c("Actors", "Categories", 
                    "Trust to develop AI", "Trust to manage AI")) %>%
  column_spec(1, width = "4cm")
```

## AI policy and China

### Survey experiment: comparing perceptions of U.S. vs. China AI research and development 

#### Procedure

In this survey experiment, we ask respondents to consider either the U.S. or China's status in AI research and development (R&D). Respondents are asked the following:

>Compared with other industrialized countries, how would you rate [the U.S./China] in AI research and development?

The responses are coded in the following way: 

- 3 = Best in the world
- 2 = Above average
- 1 = Average
- 0 = Below average

#### Results and discussion

The figure below summarizes the responses by country. The regression table presents the estimated difference in respondents' perceptions of the two countries' status in AI research and development. 

Americans do not perceive the U.S. to be better at AI R&D than China. According to our regression analysis, the respondents think China is better than the U.S. Ten percent of Americans think that the U.S. is the best in the world in terms of AI R&D while 7% think that about China. In contrast, 36% of Americans think that the U.S.'s AI R&D is "above average" while 45% think China's is "above average." Given the technological nature of this question, about a quarter of the respondents in each condition indicated that they don't know. 

```{r us_china, echo=FALSE, fig.height=7, fig.keep='all', fig.width=7, cache=TRUE, warning=FALSE}
# Frequency table
# Overall mean for recoding missing values
rd_overall_mean <- wtd.mean(relabel_var(ifelse(is.na(d$Q12a), d$Q12b, d$Q12a),
                                        c(1:5, 8, 9), c(3, 2, 1, 0, NA, NA, NA)),
                            weights = d$survey_weights, na.rm = TRUE)
# U.S. 
us_rd_value_table <- catvar_func(
  outcome = label(d0$Q12a),
  outcome_var = d$Q12a,
  label_var = d0$Q12a,
  output_type = "value_table",
  shown = d$q12a_treat == 1,
  num_missing = 8,
  num_DK = 5,
  new_values = c(3, 2, 1, 0, NA, NA, NA),
  missing_recode = rd_overall_mean
  )  
us_rd_value_table$num <- c(3, 2, 1, 0, 8, 9)
us_rd_value_table$country <- "U.S."
# China
china_rd_value_table <- catvar_func(
  outcome = label(d0$Q12b),
  outcome_var = d$Q12b,
  label_var = d0$Q12b,
  output_type = "value_table",
  shown = d$q12a_treat == 2,
  num_missing = 8,
  num_DK = 5,
  new_values = c(3, 2, 1, 0, NA, NA, NA),
  missing_recode = rd_overall_mean
  )  
china_rd_value_table$num <- c(3, 2, 1, 0, 8, 9)
china_rd_value_table$country <- "China"
# Combine the data
rd_value_table <- rbind(us_rd_value_table, china_rd_value_table)

# Numerical values
# U.S.
us_rd_value_sum <- catvar_func(
  outcome = label(d0$Q12a),
  outcome_var = d$Q12a,
  label_var = d0$Q12a,
  output_type = "num_value",
  shown = d$q12a_treat == 1,
  num_missing = 8,
  num_DK = 5,
  new_values = c(3, 2, 1, 0, NA, NA, NA),
  missing_recode = rd_overall_mean
  )  
us_rd_value_sum$country <- "U.S."
# China
china_rd_value_sum <- catvar_func(
  outcome = label(d0$Q12b),
  outcome_var = d$Q12b,
  label_var = d0$Q12b,
  output_type = "num_value",
  shown = d$q12a_treat == 2,
  num_missing = 8,
  num_DK = 5,
  new_values = c(3, 2, 1, 0, NA, NA, NA),
  missing_recode = rd_overall_mean
  )  
china_rd_value_sum$country <- "China"
# combine the num_value datasets
rd_value_sum <- rbind(us_rd_value_sum, china_rd_value_sum)
# Set factor levels
rd_value_table$country <- factor(rd_value_table$country, levels = c("U.S.", "China"))
rd_value_sum$country <- factor(rd_value_sum$country, levels = c("U.S.", "China"))
# Make the graph
ggplot() +
  geom_bar(data = rd_value_table, aes(x = num, y = Prop), stat = "identity",
           alpha = 0.55) +
  geom_text(data = rd_value_table, aes(x = num, 
                                           label = paste0(roundfunc(Prop*100, 0), "%")), 
            y = 0.02) +
  scale_x_continuous(breaks = rd_value_table$num[order(rd_value_table$num)],
    labels = str_wrap(rd_value_table$labels[order(rd_value_table$num)], 
                      width = 15)) +
  facet_grid(country~group, scales = "free_x", space = "free_x") + theme_bw() +
  geom_point(data = rd_value_sum, aes(x = num), 
             y = max(rd_value_table$Prop)+0.02) +
  geom_errorbarh(data = rd_value_sum, aes(x = num, xmin = qnorm(0.025)*se + num,
                                       xmax = qnorm(0.975)*se + num,
                                       y = max(rd_value_table$Prop)+0.02)) + 
  geom_text(data = rd_value_sum, aes(x = num, label = sum_stat,
                                       y = max(rd_value_table$Prop)+0.05)) +
  scale_y_continuous(labels = scales::percent, 
                     limits = c(0, max(rd_value_table$Prop)+0.05)) +
  xlab("Outcomes") + ylab("Percentage of respondents") + 
  ggtitle("Comparing U.S. and China's AI research and development")

# Clean the data for regression analysis
d$Q12ab_clean <- relabel_var(ifelse(is.na(d$Q12a) | d$Q12a == 9, d$Q12b, d$Q12a),
                                        c(1:5, 8, 9), c(3, 2, 1, 0, NA, NA, NA))
d$Q12ab_missing <- is.na(d$Q12ab_clean)
d$Q12ab_clean[is.na(d$Q12ab_clean)] <- rd_overall_mean
d$q12a_treat_clean <- relabel_var(d$q12a_treat, c(1, 2, 8, 9), c("U.S.", "China", NA, NA))
# Attrition check
# Attrition rates by experimental groups
d$Q12_ab <- ifelse(is.na(d$Q12a) | d$Q12a == 9, d$Q12b, d$Q12a)
q12ab_attrition <- d %>% group_by(q12a_treat_clean) %>% dplyr::summarise(
  attrition_prop = mean(Q12ab_missing)*100,
  DK_prop = mean(Q12_ab == 5)*100
) 
q12ab_attrition$missing_prop <- q12ab_attrition$attrition_prop - q12ab_attrition$DK_prop
kable(x = q12ab_attrition, format = "pandoc",
      caption = "Survey experiment attrition check: comparing U.S. and China's AI research and development", col.names = c("Experimental condition", "Percent DK/missing", "Percent DK", "Percent missing"), digits = 2)
# Attrition check: regression results 
if (sum(q12ab_attrition$attrition_prop > 0) > 0) {
  regression_output(formula = "Q12ab_missing ~ q12a_treat_clean", 
                  variable_names = c("(Intercept)", "U.S."),
                  caption = "Survey experiment attrition check: comparing U.S. and China's AI research and development") 
}
# Get the regression output 
if (sum(q12ab_attrition$attrition_prop > 10) > 0) {
  regression_output(formula = "Q12ab_clean ~ q12a_treat_clean + scale(Q12ab_missing) + q12a_treat_clean:scale(Q12ab_missing)", 
                  variable_names = c("(Intercept)", "U.S.", "DK/missing", "U.S. x DK/missing"),
                  caption = "Survey experiment results: comparing U.S. and China's AI research and development (controlling for DK/missing responses)", 
                  kable_output = c(1:2, 5))  
} else {
  regression_output(formula = "Q12ab_clean ~ q12a_treat_clean", 
                  variable_names = c("(Intercept)", "U.S."),
                  caption = "Survey experiment results: comparing U.S. and China's AI research and development")
}
```

### Survey experiment: U.S.-China arms race

#### Procedure

In this survey experiment, respondents are randomly assigned to consider different arguments about an U.S.-China arms race. All respondents are given the following prompt:

>Leading analysts believe that an “AI arms race” is beginning, in which the U.S. and China are investing billions of dollars to develop powerful AI systems for surveillance, autonomous weapons, cyber operations, propaganda, and command and control systems.

Those in the treatment condition are told they will read a short news article. The three treatments are:

1. **Pro-nationalist treatment**: The U.S. should invest heavily in AI to stay ahead of China; quote from a senior National Security Council official

2. **Risks of arms race treatment**: The U.S.-China arms race could increase the risk of a catastrophic war; quote from Elon Musk

3. **One common humanity treatment**: The U.S.-China arms race could increase the risk of a catastrophic war; quote from Stephen Hawking about using AI for the good of all people rather than destroying civilization 

The exact wording of these treatments are found in [the Appendix](#arms_race_exp).

Respondents are asked to consider two statements and indicate whether they agree or disagree with them. The two statements are:

- The U.S. should invest more in AI military capabilities to make sure it doesn't fall behind China, even if doing so may exacerbate the AI arms race.

- The U.S. should work hard to cooperate with China to avoid the dangers of an AI arms race, even if doing so requires giving up some of the U.S.'s advantages. Cooperation could include collaborations between American and Chinese AI research labs, or the U.S. and China creating and committing to common safety standards for AI.

#### Results and discussion

In the first figure below, the mean outcome, the standard error, the 95% confidence interval, and the number of responses are shown for each experimental group. The next figure shows the estimated treatment effect (difference in outcome between each treatment group and the control group). 

The final figure summarizes the difference in responses to Statement 2 and Statement 1 by experimental group. Positive values mean greater agreement with Statement 2 than with Statement 1; negative values mean greater agreement with Statement 1 than with Statement 2. 

Americans, in general, weakly agree that the U.S. should invest more in AI military capabilities _and_ should cooperate with China to avoid the dangers of an AI arms race. Respondents assigned to Treatment 2, the risks of arms race treatment, indicate significantly greater agreement with Statement 2 (pro-cooperation) than Statement 1 (investing in AI military capabilities). In contrast, respondents assigned to the other conditions indicate similar levels of agreement with both statements.

After estimating the treatment effects, we find that the experimental messages, overall, do little to change the respondents' preferences, with one exception. Treatment 2 decreases respondents' agreement with the statement that the U.S. should invest more in AI military capabilities by 27%.

```{r arms_race, echo=FALSE, fig.height=4.5, fig.keep='all', fig.width=7, warning=FALSE, cache=TRUE}

# Overall means for the two outcomes (to recode the missing values)
Q12_overall_mean <- wtd.mean(relabel_var(d$Q12, c(1:6, 8, 9),
                                         c(2, 1, 0, -1, -2, NA, NA, NA)),
                             weights = d$survey_weights, na.rm = TRUE)
Q13_overall_mean <- wtd.mean(relabel_var(d$Q13, c(1:6, 8, 9),
                                         c(2, 1, 0, -1, -2, NA, NA, NA)),
                             weights = d$survey_weights, na.rm = TRUE)

# Function to generate the results
china_exp <- function(varname, outcome, exp_group, output_type = "num_value",
                      missing_recode,
                      new_values = c(2, 1, 0, -1, -2, NA, NA, NA)) {
  return(data.frame(catvar_func(
  outcome = outcome,
  outcome_var = d[,varname],
  label_var = d0[,varname],
  output_type = output_type,
  shown = (d$q12_treat == exp_group),
  num_missing = 8,
  num_DK = 6,
  new_values = new_values,
  missing_recode = missing_recode
  ), exp_group = exp_group))  
}
# Statement 1
exp_invest <- lapply(1:4, china_exp, 
                           varname = "Q12", missing_recode = Q12_overall_mean,
                           new_values = c(2, 1, 0, -1, -2, NA, NA, NA),
                     outcome = "Agreement with statement that U.S. should invest more in AI military capabilities",
       output_type = "num_value") %>% do.call(what = rbind)
# Statement 2
exp_cooperate <- lapply(1:4, china_exp, 
                           varname = "Q13", missing_recode = Q13_overall_mean,
                           new_values = c(2, 1, 0, -1, -2, NA, NA, NA),
                     outcome = "Agreement with statement that US should work hard to cooperate with China to avoid dangers of AI arms race",
       output_type = "num_value") %>% do.call(what = rbind)
# Clean up the data
ar_groups <- c("Control", "Treatment 1: Pro-nationalist","Treatment 2: Risks of arms race", 
  "Treatment 3: One common humanity")
exp_l <- data.frame(exp_group = 1:4, exp_group_l = ar_groups)
exp_outcome <- merge(x = rbind(exp_invest, exp_cooperate), y = exp_l, all.x = TRUE)
exp_outcome$sum_stat <- gsub(pattern = "; ", replacement = ";\n", exp_outcome$sum_stat)
exp_outcome$exp_group_l <- factor(x = exp_outcome$exp_group_l, 
                                  levels = rev(levels(exp_outcome$exp_group_l)))
# Make the plot
ggplot(data = exp_outcome, aes(x = exp_group_l, y = num, 
                                 ymin = qnorm(0.025) * se + num,
                                 ymax = qnorm(0.975) * se + num)) +
  geom_pointrange(position = position_dodge(width = 0.9)) + 
  geom_text(aes(label = sum_stat), nudge_x = 0.4, alpha = 0.6) +
  coord_flip() + 
  scale_x_discrete(labels = function(x) str_wrap(x, width = 20),
                         name = "Experimental groups", expand = c(0.1, 0)) + 
  scale_y_continuous(
    name = "Agreement/disagreement with statement\n(-2 = Strongly disagree; 2 = Strongly agree)") + ggtitle("Responses from U.S.-China arms race survey experiment") +
  facet_grid(~outcome, labeller = label_wrap_gen(width = 45)) + theme_bw()
```

```{r arms_race_regression, echo=FALSE, fig.height=3.5, fig.keep='all', fig.width=7, cache=TRUE, warning=FALSE}
# Regression results
# Clean up the data
d$q12_treat_clean <- relabel_var(d$q12_treat, c(1:4, 8, 9),
                                 c(ar_groups, NA, NA))
# Statement 1
d$Q12_clean <-
  relabel_var(
    old_var = d$Q12,
    old_labels = c(1:6, 8, 9),
    new_labels = c(2, 1, 0,-1,-2, NA, NA, NA)
  )
d$Q12_missing <- is.na(d$Q12_clean)
d$Q12_clean[is.na(d$Q12_clean)] <- Q12_overall_mean
# Attrition check
# Attrition rates by experimental groups
q12_attrition <-
  d %>% group_by(q12_treat_clean) %>% dplyr::summarise(attrition_prop = mean(Q12_missing) *
                                                         100,
                                                       DK_prop = mean(Q12 == 6) * 100)
q12_attrition$missing_prop <-
  q12_attrition$attrition_prop - q12_attrition$DK_prop
kable(
  x = q12_attrition, format = "pandoc",
  caption = "Survey experiment attrition check: agreement with statement that U.S. should invest more in AI military capabilities",
  col.names = c(
    "Experimental condition",
    "Percent DK/missing",
    "Percent DK",
    "Percent missing"
  ),
  digits = 2
)

# Attrition check: regression results
if (sum(q12_attrition$attrition_prop > 0) > 0) {
  regression_output(
    formula = "Q12_missing ~ q12_treat_clean",
    variable_names = c("(Intercept)", ar_groups[2:4]),
    caption = "Survey experiment attrition check: agreement with statement that U.S. should invest more in AI military capabilities"
  )
}


# Statement 2
d$Q13_clean <-
  relabel_var(
    old_var = d$Q13,
    old_labels = c(1:6, 8, 9),
    new_labels = c(2, 1, 0,-1,-2, NA, NA, NA)
  )
d$Q13_missing <- is.na(d$Q13_clean)
d$Q13_clean[is.na(d$Q13_clean)] <- Q13_overall_mean
d$q13_treat_clean <- d$q12_treat_clean
# Attrition rates by experimental groups
q13_attrition <-
  d %>% group_by(q13_treat_clean) %>% dplyr::summarise(attrition_prop = mean(Q13_missing) *
                                                         100,
                                                       DK_prop = mean(Q13 == 6) * 100)
q13_attrition$missing_prop <-
  q13_attrition$attrition_prop - q13_attrition$DK_prop
kable(
  x = q13_attrition, format = "pandoc",
  caption = "Survey experiment attrition check: agreement with statement that US should work hard to cooperate with China to avoid dangers of AI arms race",
  col.names = c(
    "Experimental condition",
    "Percent DK/missing",
    "Percent DK",
    "Percent missing"
  ),
  digits = 2
)

# Attrition check: regression results
if (sum(q13_attrition$attrition_prop > 0) > 0) {
  regression_output(
    formula = "Q13_missing ~ q13_treat_clean",
    variable_names = c("(Intercept)", ar_groups[2:4]),
    caption = "Survey experiment attrition check: agreement with statement that US should work hard to cooperate with China to avoid dangers of AI arms race"
  )
}

# Linear regression function
survey_experiment_func <-
  function(outcome_var, exp_group, outcome) {
    survey_weights <-
      d$survey_weights[d$q12_treat_clean %in% c("Control", exp_group)]
    if (outcome_var == "Q12_clean") {
      d$q12_treatment <- d$q12_treat_clean == exp_group
      if (sum(q12_attrition$attrition_prop[q12_attrition$q12_treat_clean %in%
                                           c("Control", exp_group)] > 0.1) > 0) {
        my_formula <-
          as.formula("Q12_clean ~ q12_treatment + scale(Q12_missing) + q12_treatment:scale(Q12_missing)")
      } else {
        my_formula <- as.formula("Q12_clean ~ q12_treatment")
      }
      summary(lm(my_formula,
                 data = d[d$q12_treat_clean %in% c("Control", exp_group), ],
                 weights = survey_weights), robust = TRUE)
      md <- summary(lm(my_formula,
                       data = d[d$q12_treat_clean %in% c("Control", exp_group), ],
                       weights = survey_weights), robust = TRUE)
    } else {
      d$q13_treatment <- d$q13_treat_clean == exp_group
      if (sum(q13_attrition$attrition_prop[q13_attrition$q13_treat_clean %in%
                                           c("Control", exp_group)] > 0.1) > 0) {
        my_formula <-
          as.formula("Q13_clean ~ q13_treatment + scale(Q13_missing) + q13_treatment:scale(Q13_missing)")
      } else {
        my_formula <- as.formula("Q13_clean ~ q13_treatment")
      }
      md <- summary(lm(my_formula,
                       data = d[d$q13_treat_clean %in% c("Control", exp_group), ],
                       weights = survey_weights), robust = TRUE)
    }
    data.frame(
      outcome = outcome,
      treatment = exp_group,
      num = md$coefficient[2, 1],
      se = md$coefficients[2, 2],
      p_value = md$coefficients[2, 4],
      N = length(survey_weights)
    )
  }


# Run the analysis
ar_est <- rbind(
  lapply(
    ar_groups[2:4],
    survey_experiment_func,
    outcome_var = "Q12_clean",
    outcome = "Agreement with statement that U.S. should invest more in AI military capabilities"
  ) %>% do.call(what = rbind),
  lapply(
    ar_groups[2:4],
    survey_experiment_func,
    outcome_var = "Q13_clean",
    outcome = "Agreement with statement that US should work hard to cooperate with China to avoid dangers of AI arms race"
  ) %>% do.call(what = rbind)
)

# Improve the aesthetics
ar_est$treatment <-
  factor(ar_est$treatment, levels = rev(levels(ar_est$treatment)))
ar_est$outcome <- str_wrap(ar_est$outcome, width = 40)
ar_est$outcome <-
  factor(ar_est$outcome, levels = unique(ar_est$outcome))
ar_est$stars <- ""
ar_est$stars[ar_est$p_value < 0.05] <- "*"
ar_est$stars[ar_est$p_value < 0.005] <- "**"
ar_est$stars[ar_est$p_value < 0.001] <- "***"
ar_est$new_text <- paste0(sprintf("%.2f", round(ar_est$num, 2)),
                          " (",
                          sprintf("%.2f", round(ar_est$se, 2)),
                          ")",
                          ar_est$stars)

ggplot(data = ar_est,
       aes(
         x = treatment,
         y = num,
         ymin = qnorm(0.025) * se + num,
         ymax = qnorm(0.975) * se + num
       )) +
  geom_hline(yintercept = 0,
             linetype = 2,
             alpha = 0.5) +
  geom_pointrange(position = position_dodge(width = 0.9)) +
  geom_text(aes(label = new_text), nudge_x = 0.3, alpha = 0.6) +
  coord_flip() +
  scale_x_discrete(
    labels = function(x)
      str_wrap(x, width = 20),
    name = "Experimental groups"
  ) +
  scale_y_continuous(,
    name = "Estimated treatment effects\n(Outcomes: -2 = Strongly disagree; 2 = Strongly agree)"
  ) +
  ggtitle(str_wrap(
    "Effect estimates from U.S.-China arms race survey experiment",
    width = 60
  )) +
  facet_grid( ~ outcome, labeller = label_wrap_gen(width = 45)) + theme_bw()
```

```{r arms_race_diff, echo=FALSE, fig.height=3.5, fig.keep='all', fig.width=7, warning=FALSE, cache=TRUE}
d$Q13_12_diff <- d$Q13_clean-d$Q12_clean
d$Q13_12_diff_missing <- !d$Q13_12_diff %in% c(-4:4)

# Difference between statements

ar_diff <- function(gnum) {
  md <- summary(lm(Q13_12_diff ~ scale(Q13_12_diff_missing), data = d, 
           subset = d$q12_treat == gnum,
           weights = d$survey_weights), robust = TRUE)  
  return(data.frame(exp_group_l = gnum, num = md$coefficients[1,1], 
                    se = md$coefficients[1,2], N = length(md$residuals)))
}

ar_diff_res <- do.call(rbind, lapply(1:4, ar_diff))
ar_diff_res$exp_group_l <- ar_groups
ar_diff_res$sum_stat <- paste0(roundfunc(ar_diff_res$num), " (",
                               roundfunc(ar_diff_res$se), "); N=", ar_diff_res$N)
ar_diff_res$exp_group_l <- factor(ar_diff_res$exp_group_l, levels = rev(ar_groups))
# Make the plot
ggplot(data = ar_diff_res, aes(x = exp_group_l, y = num, 
                                 ymin = qnorm(0.025) * se + num,
                                 ymax = qnorm(0.975) * se + num)) +
  geom_hline(yintercept = 0, alpha = 0.5, linetype = 2) +
  geom_pointrange(position = position_dodge(width = 0.9)) + 
  geom_text(aes(label = sum_stat), nudge_x = 0.4, alpha = 0.6) +
  coord_flip() + 
  scale_x_discrete(labels = function(x) str_wrap(x, width = 20),
                         name = "Experimental groups") + 
  scale_y_continuous(
    name = "Response to Statement 2 (cooperate with China) -\nResponse to Statement 1 (invest in AI military capabilities)") + 
  ggtitle("Difference in response to the two statements by experimental group") + 
  theme_bw()
```

### Issue areas for possible U.S.-China cooperation 

#### Procedure 

This question investigates issue areas where Americans perceive likely U.S.-China cooperation. Each respondent is randomly assigned to consider three out of five AI governance challenges. For each challenge, he or she is asked, "For the following issues, how likely is it that the U.S. and China can cooperate?". 

The five AI governance challenges include:

- Prevent AI cyber attacks against governments, companies, organizations, and individuals
- Prevent AI-assisted surveillance from violating privacy and civil liberties
- Make sure AI systems are safe, trustworthy, and aligned with human values
- Ban the use of lethal autonomous weapons
- Guarantee a good standard of living for those who lose their jobs to automation

Respondents indicate the likelihood of cooperation using the following multiple choices:

- Very unlikely: less than 5% chance
- Unlikely: 5-20% chance
- Somewhat unlikely: 20-40% chance
- Equally likely as unlikely: 40-60% chance
- Somewhat likely: 60-80% chance
- Likely: 80-95% chance 
- Very likely: more than 95% chance

For the analysis, the multiple-choice answers are converted to the median value of each answer choice range. For instance, the "very unlikely" answer is converted to 2.5%.

#### Results and discussion

The figure below shows the mean perceived likelihood of cooperation on each of the issue areas.

On each of these issues, Americans -- on average -- think that cooperation is less likely than not. U.S.-China cooperation on value alignment is perceived to be the most likely (48% mean likelihood). Cooperation to prevent AI-assisted surveillance that violates privacy and civil liberties is perceived to be the least likely (40% mean likelihood).

```{r coop_china, echo=FALSE, fig.height=3.5, fig.keep='all', fig.width=7, warning=FALSE, cache=TRUE}
# Overall mean to recode the missing data
coop_overall_mean <- wtd.mean(relabel_var(as.numeric(unlist(d[,paste0("Q14_", 1:5)])), 
            c(1:8, 98, 99), c(mc_p_med/100, NA, NA, NA)), 
         weights = rep(d$survey_weights, 5), na.rm = TRUE)

# Function to generate the results
china_coop <- function(item_num, output_type = "num_value",
                      new_values = c(mc_p_med/100, NA, NA, NA)) {
  # Generate the "shown" variable
  shown_variable <- d[,paste0("Q14_", item_num)] != 99
  # Analysis function
  return(catvar_func(
    outcome = label(x = d0[,paste0("Q14_", item_num)]),
    outcome_var = d[,paste0("Q14_", item_num)],
    label_var = d0[,paste0("Q14_", item_num)],
    output_type = output_type,
    shown = shown_variable,
    num_missing = 98,
    num_DK = 8, missing_recode = coop_overall_mean,
    new_values = new_values))
}
# Generate the data
china_cd_num_value <- do.call(rbind, lapply(1:5, china_coop))
# Clean the data
china_cd_num_value$outcome <- c("Prevent AI cyber attacks against governments, companies, organizations, and individuals", 
                                "Prevent AI-assisted surveillance from violating privacy and civil liberties", 
                                "Make sure AI systems are safe, trustworthy, and aligned with human values", "Ban the use of lethal autonomous weapons", 
                                "Guarantee a good standard of living for those who lose their jobs to automation")
# Make the plot
china_cd_num_value$sum_stat <- paste0(roundfunc(china_cd_num_value$num*100), " (",
                                      roundfunc(china_cd_num_value$num*100), "); N = ",
                                      china_cd_num_value$N)
ggplot(data = china_cd_num_value, aes(x = outcome, y = num, 
                                 ymin = qnorm(0.025) * se + num,
                                 ymax = qnorm(0.975) * se + num)) +
  geom_pointrange(position = position_dodge(width = 0.9)) + 
  geom_text(aes(label = sum_stat), nudge_x = 0.3, alpha = 0.6) +
  coord_flip() + 
  scale_x_discrete(labels = function(x) str_wrap(x, width = 35),
                         name = "Issue areas") + 
  scale_y_continuous(limits = c(0.275, 0.525), labels = scales::percent,
    name = "Perceived likelihood of cooperation (percentage points)") + 
  ggtitle("Issue areas for possible U.S.-China cooperation") +
  theme_bw()
```

## Trend across time: job creation or job loss

This question seeks to understand how Americans' attitudes towards automation changes over time. Between 1983 and 2003, U.S. National Science Foundation conducted eight surveys that asked respondents the following:

>In general, computers and factory automation will create more jobs than they will eliminate.  Do you strongly agree, agree, disagree, or strongly disagree?

Our survey continues this time trend study by posing a similar -- but updated -- question:

>Do you strongly agree, agree, disagree, or strongly disagree with the statement below?
In general, automation and AI will create more jobs than they will eliminate.

The responses are coded in the following way:

- -2 = Strongly disagree
- -1 = Disagree
- 1 = Agree
- 2 = Strongly agree 

### Survey experiment: future time frame

#### Procedure 

Our survey question also addresses the chief ambiguity of the original question: lack of a future time frame. We use a survey experiment to help resolve this ambiguity by randomly assigning respondents to one of four conditions. Besides the control condition in which we do not specify a future time frame, we also created three treatment conditions that defined the future time frame:

- In 10 years
- In 20 years
- In 50 years

#### Results and discussion

The figure below summarizes the responses by experimental group. The regression table presents the estimated differences in responses between the control and experimental groups.

On average, Americans disagree with the statement more than they agree with it, although about a quarter of respondents give a "don't know" response in each experimental group. Those assigned to the 10 year time frame indicate greater disagreement than those assigned to the control condition of no specified time frame (two-sided $p$-value = 0.04). Agreement with the statement appears to increase with the time frame, but there exist no significant differences between the responses of differing time frame.  

This is puzzling from the perspective that AI and robotics will increasingly automate human performed tasks. Such a perspective would expect more agreement as one looks further into the future. One hypothesis to explain this pattern is that respondents believe the disruption from automation is greater in the coming ten years, then it will be over the longer run once institutions adapt. Another hypothesis is that asking about the next ten years makes this question more concrete, whereas in the more abstract framing different reasoning processes may operate. The effects are sufficiently uncertain that we cannot conclude with confidence that agreement goes up with time frame; however, we can conclude with confidence that agreement does not go strongly down with time frame (e.g., we can reject that average agreement decreases by one point going from 10 to 50 years frame).

```{r jobs_loss, echo=FALSE, fig.height=3.5, fig.keep='all', fig.width=7, warning=FALSE, cache=TRUE}
# Overall mean for recoding the missing values
job_creation_overall_mean <- 
  wtd.mean(relabel_var(d$Q15, c(1:5, 8, 9), c(2, 1, -1, -2, NA, NA, NA)),
           weights = d$survey_weights, na.rm = TRUE)

# Function to generate the results
job_creation <- function(varname, exp_group, output_type = "num_value",
                      new_values = c(2, 1, -1, -2, NA, NA, NA)) {
  return(data.frame(catvar_func(
  outcome = "Agreement with the statement that automation will lead to job creation",
  outcome_var = d[,varname],
  label_var = d0[,varname],
  output_type = output_type,
  shown = (d$q15_treat == exp_group),
  num_missing = 5,
  num_DK = 8,
  new_values = new_values, 
  missing_recode = job_creation_overall_mean,
  ), exp_group = names(val_labels(d0$q15_treat))[exp_group]))  
}
job_exp <- lapply(1:4, job_creation, varname = "Q15") %>% do.call(what = rbind)
job_exp$exp_group <- factor(job_exp$exp_group, levels = rev(levels(job_exp$exp_group)))
# Make the plot
ggplot(data = job_exp, aes(x = exp_group, y = num, 
                                 ymin = qnorm(0.025) * se + num,
                                 ymax = qnorm(0.975) * se + num)) +
  geom_pointrange(position = position_dodge(width = 0.9)) + 
  geom_text(aes(label = sum_stat), nudge_x = 0.3, alpha = 0.6) +
  coord_flip() + 
  scale_x_discrete(labels = function(x) str_wrap(x, width = 35),
                         name = "Time frame into the future") + 
  scale_y_continuous(
    name = "Agreement with the statement\n(-2 = Strongly disagree; 2 = Strongly agree)") + 
  ggtitle(str_wrap("Agreement with the statement that automation will create more jobs than it will eliminate", width = 65)) +
  theme_bw()

# Clean the data for regression analysis
d$Q15_clean <- relabel_var(d$Q15, c(1:5, 8, 9), c(2, 1, -1, -2, NA, NA, NA))
d$Q15_missing <- is.na(d$Q15_clean)
d$Q15_clean[is.na(d$Q15_clean)] <- job_creation_overall_mean
d$q15_treat_clean <- relabel_var(d$q15_treat, c(1:4, 8, 9), 
                                 c("No time frame", "10 years", 
                                   "20 years", "50 years", NA, NA))
d$q15_treat_clean <- factor(d$q15_treat_clean, levels = c("No time frame", "10 years", 
                                   "20 years", "50 years"))

# Attrition rates by experimental groups
q15_attrition <-
  d %>% group_by(q15_treat_clean) %>% dplyr::summarise(attrition_prop = mean(Q15_missing) *
                                                         100,
                                                       DK_prop = mean(Q15 == 5) * 100)
q15_attrition$missing_prop <-
  q15_attrition$attrition_prop - q15_attrition$DK_prop
kable(
  x = q15_attrition, format = "pandoc",
  caption = "Survey experiment attrition check: future time frame",
  col.names = c(
    "Experimental condition",
    "Percent DK/missing",
    "Percent DK",
    "Percent missing"
  ),
  digits = 2
)

# Attrition check: regression results
if (sum(q15_attrition$attrition_prop > 0) > 0) {
  regression_output(
    formula = "Q15_missing ~ q15_treat_clean",
    variable_names = c("(Intercept)", "10 years", "20 years", "50 years"),
    caption = "Survey experiment attrition check: future time frame"
  )
}

# Regression analysis
if (sum(q15_attrition$attrition_prop > 0.1) > 0) {
  d$q15_treat_10y <- d$q15_treat_clean == "10 years"
  d$q15_treat_20y <- d$q15_treat_clean == "20 years"
  d$q15_treat_50y <- d$q15_treat_clean == "50 years"
  regression_output(formula = "Q15_clean ~ q15_treat_10y + q15_treat_20y + q15_treat_50y + scale(Q15_missing) + q15_treat_10y:scale(Q15_missing) + q15_treat_20y:scale(Q15_missing) +q15_treat_50y:scale(Q15_missing)",
                  variable_names = c("(Intercept)", "10 years", "20 years", "50 years",
                                     "DK/missing", "10 years x DK/missing",
                                     "20 years x DK/missing", "50 years x DK/missing"),
                  caption = "Survey experiment results: future time frame (controlling for DK/missing responses)", kable_output = c(1:4, 9))  
  
} else {
  regression_output(formula = "Q15_clean ~ q15_treat_clean",
                  variable_names = c("(Intercept)", "10 years", "20 years", "50 years"),
                  caption = "Survey experiment results: future time frame")  
}

md_Q15 <- regression_output(formula = "Q15_clean ~ q15_treat_10y + q15_treat_20y + q15_treat_50y + scale(Q15_missing) + q15_treat_10y:scale(Q15_missing) + q15_treat_20y:scale(Q15_missing) +q15_treat_50y:scale(Q15_missing)",
                  variable_names = c("(Intercept)", "10 years", "20 years", "50 years",
                                     "DK/missing", "10 years x DK/missing",
                                     "20 years x DK/missing", "50 years x DK/missing"),
                  caption = "Survey experiment results: future time frame", 
                  output_model = TRUE) 

y10_20 <- linearHypothesis(md_Q15, c("q15_treat_10yTRUE = q15_treat_20yTRUE"), 
                        white.adjust = TRUE)
y10_50 <- linearHypothesis(md_Q15, c("q15_treat_10yTRUE = q15_treat_50yTRUE"), 
                 white.adjust = TRUE)
y20_50 <- linearHypothesis(md_Q15, c("q15_treat_20yTRUE = q15_treat_50yTRUE"), 
                           white.adjust = TRUE)

coef_equ_func <- function(dataset, test_text) {
  data.frame(test = test_text, f_stat = paste0("F(",
                                                         dataset$Df[2], ", ", 
                                                         dataset$Res.Df[2], ") = ",
                                                         roundfunc(dataset$F[2])),
           p_value = roundfunc(dataset$`Pr(>F)`[2]))  
}

coef_equ_timeline <- rbind(coef_equ_func(y10_20, "10 years = 20 years"),
                  coef_equ_func(y10_50, "10 years = 50 years"),
                  coef_equ_func(y20_50, "20 years = 50 years"))
kable(coef_equ_timeline, caption = "Testing coefficients for time frames are equivalent",
      row.names = FALSE, col.names = c("Tests", "F-statistic", "p-value"), 
      format = "pandoc")

```

#### Extending the time trend

The figure below summarizes the historical polling data while adding in data points from our survey. The "strongly agree" and "agree" responses are grouped together; likewise, the "strongly disagree" and "agree" responses are grouped together. 

The percent of Americans today who disagree with the statement is on par with historical levels. Nevertheless, the percent who agree with the statement has decreased by 12 percentage points since 2003 while the percent who responded "don't know" has increased by 18 percentage points since 2003. 

There are three possible reasons for these observed changes. First, we have updated the question to ask about "automation and AI" instead of "computers and factory automation." The technologies we asked about could impact a wider swath of the economy; therefore, respondents may be more uncertain about their impact on the labor market. Second, there is a difference in survey mode between the historical data and our data. The NSF surveys were conducted via telephone while our survey is conducted online. Some previous research has shown that online surveys, compared with telephone surveys, produce a greater percentage of "don't know" responses [@nagelhout2010web; @bronner2007live]; however, other studies have shown that online surveys cause no such effect [@shin2012survey; @bech2009differential]. Third, the changes in response could have arisen due to the actual changes in respondents' perceptions over time. 

```{r jobs_compare, echo=FALSE, fig.height=5, fig.keep='all', fig.width=7, warning=FALSE, cache=TRUE}
roper <- read.csv("~/Google Drive/AI Public Opinion Surveys/AnnualReview/data/roper_survey_data.csv", stringsAsFactors=FALSE)
nsf <- roper[roper$QuestionTxt == "In general, computers and factory automation will create more jobs than they will eliminate.  Do you strongly agree, agree, disagree, or strongly disagree?", ]
# get the survey year
nsf$survey_year <- rep(NA, nrow(nsf))
for (i in 1:nrow(nsf)) {
  nsf$survey_year[i] <- as.numeric(strsplit(nsf$EndDate[i], "/")[[1]][3])
}
nsf$RespPct[nsf$RespPct == "*"] <- 0.5
nsf$RespPct <- as.numeric(nsf$RespPct)
# Add up agrees and disagrees
nsf <- nsf %>% group_by(QuestionID) %>% dplyr::summarize(
  Agree = sum(RespPct[RespTxt %in% c("Strongly agree", "Agree")]),
  Disagree = sum(RespPct[RespTxt %in% c("Strongly disagree", "Disagree")]),
  DK = sum(RespPct[RespTxt == "Don't know"]),
  survey_year = mean(survey_year)
) 
# Melt the data
nsf <- reshape2::melt(nsf, id = c("QuestionID", "survey_year"))
nsf$variable <- as.character(nsf$variable)
nsf$variable[nsf$variable == "DK"] <- "Don't know"
nsf$variable[nsf$variable == "Agree"] <- "Strongly agree/Agree"
nsf$variable[nsf$variable == "Disagree"] <- "Strongly disagree/Disagree"
nsf$variable <- factor(nsf$variable, 
                                  levels = unique(nsf$variable))
nsf$survey_type <- "NSF"
# Add in the YouGov data
job_exp_nsf <- lapply(1:4, job_creation, varname = "Q15", 
                      output_type = "value_table") %>% do.call(what = rbind) %>% group_by(exp_group) %>% dplyr::summarise(
    Agree = sum(Prop[labels %in% c("2. Strongly agree", "1. Agree")])*100,
    Disagree = sum(Prop[labels %in% c("-2. Strongly disagree", "-1. Disagree")])*100,
    DK = sum(Prop[labels == "Don't know"])*100,
    survey_year = 2018,
    survey_type = "Gov-AI"
)
names(job_exp_nsf)[names(job_exp_nsf) == "exp_group"] <- "QuestionID"
job_exp_nsf <- reshape2::melt(job_exp_nsf, id = c("QuestionID", 
                                                  "survey_year", "survey_type"))
job_exp_nsf$variable <- as.character(job_exp_nsf$variable)
job_exp_nsf$variable[job_exp_nsf$variable == "DK"] <- "Don't know"
job_exp_nsf$variable[job_exp_nsf$variable == "Agree"] <- "Strongly agree/Agree"
job_exp_nsf$variable[job_exp_nsf$variable == "Disagree"] <- "Strongly disagree/Disagree"
job_exp_nsf$variable <- factor(job_exp_nsf$variable, 
                                  levels = unique(job_exp_nsf$variable))
nsf <- rbind(nsf, job_exp_nsf)
nsf$noexp <- !nsf$QuestionID %in% c("10 years", "20 years", "50 years")
# Make the graph
ggplot() + 
  geom_point(data = nsf[nsf$noexp,], 
             aes(x = survey_year, y = value/100, color = variable)) + 
  geom_point(data = nsf[!nsf$noexp,], 
             aes(x = survey_year, y = value/100, shape = QuestionID, color = variable)) + 
  geom_line(data = nsf[nsf$noexp,], 
            aes(x = survey_year, y = value/100, color = variable)) +
  geom_text(data = nsf[nsf$noexp,], 
            aes(x = survey_year, y = value/100, label = paste0(roundfunc(value, 0),"%")),
            size = 3, nudge_y = 0.02) +
  scale_x_continuous("Survey year",
                     breaks = seq(1980, 2020, by = 5)) + 
  scale_color_discrete(name = "Responses") +
  scale_shape_manual(name = "Time frames for experimental conditions", 
                     values = c(22, 23, 24)) + 
  scale_y_continuous(name = "Percent of respondents", labels = scales::percent) + 
  theme_bw() + theme(legend.position = "bottom", legend.direction = "vertical") +
  ggtitle(str_wrap("Response to statement that automation will create more jobs than it will eliminate (historical data from National Science Foundation surveys)", width = 75))

```


## High-level machine intelligence

The next set of questions concerns respondents' perceptions of high-level machine intelligence.

### Forecasting timeline

Respondents are asked to forecast when high-level machine intelligence will be developed. High-level machine intelligence is defined as the following:

>Say we have high-level machine intelligence when machines will be able to perform almost all tasks that are economically relevant today better than the median human (today) at each task. Note that this would include subtle common sense queries such as a travel agent would provide. Ignore tasks which are legally or culturally restricted to humans, such as being a jury member.

Note that our definition of high-level machine intelligence is equivalent to what many would consider human-level machine intelligence. 

Respondents predict the probability that high-level machine intelligence will be built in 10, 20, and 50 years using the following multiple choices:

- Very unlikely: less than 5% chance
- Unlikely: 5-20% chance
- Somewhat unlikely: 20-40% chance
- Equally likely as unlikely: 40-60% chance
- Somewhat likely: 60-80% chance
- Likely: 80-95% chance 
- Very likely: more than 95% chance

For the analysis, the multiple-choice answers are converted to the median value of each answer choice range. For instance, the "very unlikely" answer is converted to 2.5%.

We present our results in two ways. First, we show the summary statistics in a simple table. Next, to compare the public's forecasts with forecasts made by AI researchers in 2016 (see @grace2018will), we aggregated the respondents' forecasts using the same method. Note that @grace2018will asked about a much higher definition of high-level machine intelligence involving machines being better than all humans at all tasks.

Each respondent provides three data points for their forecast, and these are fitted to the
Gamma CDF by least squares to produce the grey CDFs. The "Aggregate Forecast" is the mean distribution over all individual CDFs (also called the "mixture" distribution). The confidence interval is generated by bootstrapping (clustering on respondents) and plotting the 95% interval for estimated probabilities at each year. Please see the code and [@grace2018will] for details. Note that survey weights are not used in this analysis due to problems incorporating survey weights into the bootstrap.  

#### Results and discussion

Respondents predict that high-level machine intelligence will arrive fairly quickly. The median respondent predicts there is 54% probability of high-level machine intelligence by 2028, 70% probability by 2038, and 88% probability by 2068.

These predictions are considerably more optimistic than predictions by experts in two previous surveys. In @muller2014future, expert respondents (the "All" prediction) predict a 50% probability of high-level human intelligence by 2040 and 90% by 2075. In @grace2018will, experts predict that there is a 50% chance that high-level machine intelligence will be developed by 2061. Plotting the public's forecast with the expert forecast from @grace2018will, we see that the public perceives high-level machine intelligence arriving much sooner than experts predict.   

Results in @walsh2018expert also show that the non-experts (i.e., readers of a news article about AI) are more optimistic in their predictions of high-level machine intelligence compared with experts. While nearly 80% of non-experts think that high-level machine intelligence will be built by 2050, only 40% of experts predict that. In our survey, respondents with CS or engineering degrees give a somewhat longer timeline for the arrival of high-level machine intelligence. Nevertheless, these respondents' forecasts are more optimistic than those made by experts from @grace2018will and show considerable overlap with the overall YouGov forecast.

```{r hlmi_timeline, echo=FALSE, fig.height=5, fig.keep='all', fig.width=7, warning=FALSE, cache=TRUE, results=FALSE}

# Clean the data
# Get the overall mean for each year
hlmi_clean <- function(dataset = d) {
  # Within 10 years
  hlmi_10 <-
  relabel_var(
  dataset$Q16_1,
  old_labels = c(1:8, 98, 99),
  new_labels = c(mc_p_med / 100, NA, NA, NA)
  )
  hlmi_10_overall_mean <- wtd.mean(hlmi_10[!is.na(hlmi_10)],
  weights = dataset$survey_weights[!is.na(hlmi_10)])
  hlmi_10[is.na(hlmi_10)] <- hlmi_10_overall_mean
  # Within 20 years
  hlmi_20 <- relabel_var(
  dataset$Q16_2,
  old_labels = c(1:8, 98, 99),
  new_labels = c(mc_p_med / 100, NA, NA, NA)
  )
  hlmi_20_overall_mean <- wtd.mean(hlmi_20[!is.na(hlmi_20)],
  weights = dataset$survey_weights[!is.na(hlmi_20)])
  hlmi_20[is.na(hlmi_20)] <- hlmi_20_overall_mean
  # Within 50 years
  hlmi_50 <- relabel_var(
  dataset$Q16_3,
  old_labels = c(1:8, 98, 99),
  new_labels = c(mc_p_med / 100, NA, NA, NA)
  )
  hlmi_50_overall_mean <- wtd.mean(hlmi_50[!is.na(hlmi_50)],
  weights = dataset$survey_weights[!is.na(hlmi_50)])
  hlmi_50[is.na(hlmi_50)] <- hlmi_50_overall_mean
  return(data.frame(hlmi_10, hlmi_20, hlmi_50))
}

# HLMI full data
hlmi_full <- hlmi_clean(dataset = d)
# check monotone increase
full_mono_check <- mean(hlmi_full$hlmi_10 <= hlmi_full$hlmi_20 & 
          hlmi_full$hlmi_10 <= hlmi_full$hlmi_50 &
          hlmi_full$hlmi_20 <= hlmi_full$hlmi_50)
# HLMI people with CS/engineering degrees
hlmi_cs <- hlmi_full[d$demo_cs == "CS or engineering degree",]
# check monotone increase
cs_mono_check <- mean(hlmi_cs$hlmi_10 <= hlmi_cs$hlmi_20 & 
          hlmi_cs$hlmi_10 <= hlmi_cs$hlmi_50 &
            hlmi_cs$hlmi_20 <= hlmi_cs$hlmi_50)

# Helper function
hlmi_timeline <- function(year_num, missing_recode, output_type = "num_value",
                          shown = rep(TRUE, nrow(d))) {
  catvar_func(
    outcome = label(x = d0[,paste0("Q16_", year_num)]),
    outcome_var = d[,paste0("Q16_", year_num)],
    label_var = d0[,paste0("Q16_", year_num)],
    output_type = output_type,
    shown = shown,
    num_missing = 8,
    num_DK = 98, missing_recode = missing_recode, 
    new_values <- c(mc_p_med/100, NA, NA, NA))  
}

# Make summary statistics table
rbind(data.frame(Year = paste0("Within ", c(10, 20, 50), " years"),
           Respondents = "YouGov: all",
           rbind(summary(hlmi_full$hlmi_10, digits = 2)[2:5], 
                 summary(hlmi_full$hlmi_20, digits = 2)[2:5],
                 summary(hlmi_full$hlmi_50, digits = 2)[2:5])),
data.frame(Year = paste0("Within ", c(10, 20, 50), " years"),
           Respondents = "YouGov: with CS or engineering degree",
           rbind(summary(hlmi_cs$hlmi_10, digits = 2)[2:5], 
                 summary(hlmi_cs$hlmi_20, digits = 2)[2:5],
                 summary(hlmi_cs$hlmi_50, digits = 2)[2:5]))) %>% 
  kable(caption = "Summary statistics of high-level machine intelligence forecast",
        format = "pandoc",
         col.names = c("Year", "Respondent type",
                       "Q1", "Median", "Mean", "Q3"))

# Make the CDF graph
# Combine the data for making the CDF graph
hlmi_d <- rbind(data.frame(response.id = d$r_id, 
                           fixedprobabilities = 0, field.group = "YouGov",
           p = hlmi_full$hlmi_10, x = 10, survey_weights = d$survey_weights),
      data.frame(response.id = d$r_id, fixedprobabilities = 0, field.group = "YouGov",
           p = hlmi_full$hlmi_20, x = 20, survey_weights = d$survey_weights),
      data.frame(response.id = d$r_id, fixedprobabilities = 0, field.group = "YouGov",
           p = hlmi_full$hlmi_50, x = 50, survey_weights = d$survey_weights))

# Initial parameter values
gamma.inits <- list(
    c(4,1),
    c(4,4),
    c(4,16),
    c(4,64),
    c(4,256),
    c(4,1096),
    c(4, 4000),
    c(4, 16000))
# Fit to CDFs
gamma.fits <- hlmi_d %>% fit.all(gamma.dist.f, par_init = gamma.inits)
gamma.fits$weights <- d$survey_weights
x <- seq(0, 1000, by = 0.2)
gamma.fits.cs <- gamma.fits[d$demo_cs == "CS or engineering degree",]

# All respondents
curves <- gamma.fits %>% cum.dist(x, gamma.dist.f) %>% rename(V1=response.id, y=p)
curves$V1 <- paste0("Respondent_", curves$V1)
curves$shape <- NULL
curves$scale <- NULL
combined.curve <- curves %>% 
    group_by(x) %>% 
    summarize(y=mean(y), V1="summary_cdf")
# Combine the data for plotting
pdata <- bind_rows(
            forecasters = curves,
            combined = combined.curve, 
            .id = "source")

make.curve.matrix <- function(curves ) {
    curves %>% 
        select(V1, x, y) %>% 
        spread(x, y) %>% 
        select(-V1) %>%
        data.matrix
}
curve.matrix <- make.curve.matrix(curves)

bootstrap.curve.matrix <- function(curve.matrix, n.boot) { 
    n.r = curve.matrix %>% nrow
    bootstrap.samples = rmultinom(n = n.boot, size = n.r, 
                                  prob = rep(1, n.r)) %>% t
    bootstrap.curves = (bootstrap.samples / n.r) %*% curve.matrix
    
    flat.bootstrap.curves = bootstrap.curves %>% 
        as_data_frame %>% 
        mutate(sample=1:nrow(.)) %>% 
        gather(x, y, -sample) %>% 
        mutate(x=as.numeric(x))
    
    flat.bootstrap.curves %>% 
    group_by(x) %>% 
    summarize(lower = quantile(y, .025),upper = quantile(y, .975))
}

bs_res <- bootstrap.curve.matrix(curve.matrix, 2000)
# plot 50 forecaster's predictions (not too many)
plot_keep <- unique(pdata$V1)[sample(x = 1:length(unique(pdata$V1)), 
                                     size = 50)]

# Respondents with CS/engineering degrees
d$V1 <- paste0("Respondent_", d$r_id)
curves.cs <- subset(curves, V1 %in% d$V1[d$demo_cs == "CS or engineering degree"])
combined.curve.cs <- curves.cs %>% 
    group_by(x) %>% 
    summarize(y = mean(y), V1="summary_cdf_cs")
combined.curve.cs$source <- "cs_combined"
# Combine the data for plotting
pdata.cs <- bind_rows(
            forecasters = curves.cs,
            combined = combined.curve.cs, 
            .id = "source")
curve.matrix.cs <- make.curve.matrix(curves.cs)
bs_res.cs <- bootstrap.curve.matrix(curve.matrix = curve.matrix.cs, n.boot = 2000)


# Load in the expert forecasts
expert_bs <- read.csv("/Users/baobaozhang/Documents/AI-Public-Opinion-2/data/experts_forecasts.csv")
# Change the forecast years starting from 2018
expert_bs$x <- expert_bs$x+2016-2018 
expert_lines <- read.csv("/Users/baobaozhang/Documents/AI-Public-Opinion-2/data/experts_forecasts_lines.csv")
expert_lines$x <- expert_lines$x+2016-2018 
# Merge in the data
pdata$source <- paste0("public_", pdata$source)
pdata$V1[pdata$V1 == "summary_cdf"] <- "public_summary_cdf"
expert_lines$source <- paste0("expert_", as.character(expert_lines$source))
expert_lines$V1 <- as.character(expert_lines$V1)
expert_lines$V1[expert_lines$V1 == "summary_cdf"] <- "expert_summary_cdf"
# Combine the public and expert forecasts
all_hlmi <- rbind(pdata[,c("source", "x", "V1", "y")],
                  combined.curve.cs[,c("source", "x", "V1", "y")],
                  expert_lines[expert_lines$source == "expert_combined",
                               c("source", "x", "V1", "y")])
# Make the plot: all data
#all_hlmi$source[all_hlmi$source == "public_public_public_combined"] <- "public_combined"
#all_hlmi$source[all_hlmi$source == "public_public_public_forecasters"] <- "public_forecasters"

line.labels = c(
  public_forecasters = "Random subset of survey respondents",
  public_combined = "YouGov aggregate forecast (with 95% confidence interval)",
  cs_combined = "YouGov with CS/engineering degrees aggregate forecast (with 95% confidence interval)",
  expert_combined = "Expert aggregate forecast (with 95% confidence interval)"
 )

line.color = c(
    public_forecasters = "grey",
    public_combined = "red",
    cs_combined = "darkblue",
    expert_combined = "blue")

line.alpha = c(
    public_forecasters = .5, 
    public_combined = 1,
    cs_combined = 1,
    expert_combined = 1)

line.size = c(
    public_forecasters = .2, 
    public_combined = 0.5,
    cs_combined = 0.5,
    expert_combined = 0.5)

line.linetype = c(
    public_forecasters = 1, 
    public_combined = 1,
    cs_combined = 1,
    expert_combined = 1)

pdata.df = all_hlmi %>% 
    filter(V1 %in% plot_keep | source %in% c("expert_combined", "public_combined",
                                             "cs_combined")) %>% 
    as.data.frame()
# Make the graph 
pdata.df$source <- factor(pdata.df$source, c("public_combined", "cs_combined",
                                      "expert_combined", "public_forecasters"))
ggplot() + 
  geom_ribbon(data = bs_res, mapping = aes(x = x, ymin = lower, ymax = upper),
              alpha = 0.4, fill = "red") +
  geom_ribbon(data = expert_bs, mapping = aes(x = x, ymin = lower, ymax = upper),
              alpha = 0.4, fill = "blue") +
  geom_ribbon(data = bs_res.cs, mapping = aes(x = x, ymin = lower, ymax = upper),
              alpha = 0.4, fill = "green") +
  geom_line(data = pdata.df, 
            mapping = aes(x=x, y=y, group=V1, color=source, alpha = source, 
                  size=source, linetype=source)) +
  scale_color_manual(values=rev(line.color), name = NULL, labels=rev(line.labels)) +
  scale_alpha_manual(values=rev(line.alpha), name = NULL, labels=rev(line.labels)) +
  scale_size_manual(values=rev(line.size), name = NULL, labels=rev(line.labels)) +
  scale_linetype_manual(values=rev(line.linetype), name = NULL, labels=rev(line.labels)) + 
  xlab("Years from 2018") + 
  ylab("Probability of high-level\nmachine intelligence") + 
  coord_cartesian(xlim = c(0, 100), ylim=c(0,1)) +
  scale_x_continuous(expand=c(0,0)) + 
  scale_y_continuous(expand=c(0,0)) + 
  theme_bw() + 
  theme(legend.position = "bottom", 
        legend.direction="vertical") +
  ggtitle(str_wrap("Comparing experts and the public's forecasts of high-level machine intelligence timelines", width = 75))
```

### Support for developing high-level machine intelligence

#### Procedure 

Respondents are asked how much they support or oppose the development of high-level machine intelligence. The responses are coded in the following way:

- -2 = Strongly oppose
- -1 = Somewhat oppose
- 0 = Neither support nor oppose
- 1 = Somewhat support
- 2 = Strongly support

#### Results and discussion

The distribution of responses is displayed in the figure below. The mean outcome and the corresponding standard error are also presented.

Similar to the results about support for developing AI, Americans express mixed support for the development of high-level machine intelligence. About one-third of Americans (32%) somewhat or strongly support the development of AI while 27% somewhat or strongly oppose the development of AI. Many express a neutral attitude: 29% state that they neither support nor oppose while 12% indicate they don't know.

The correlation between support for developing AI and support for developing high-level machine intelligence is 0.62. The mean level of support for developing AI, compared with the mean level of support for developing high-level machine intelligence, is 0.24 points (SE = 0.03) higher on a five-point scale (two-sided $p$-value $<0.001$). 

```{r support_hlmi, echo=FALSE, fig.height=4, results='hide', fig.keep='all', fig.width=7, warning=FALSE, cache=TRUE}

# Get the overall mean for recoding missing data
support_hlmi_overall_mean <- wtd.mean(relabel_var(d$Q17,
                                      c(1:6, 8, 9),
                                      c(2, 1, 0, -1, -2, NA, NA, NA)),
                                      weights = d$survey_weights, na.rm = TRUE)

# Prepare the data
support_hlmi_value_table <- catvar_func(
    outcome = "Support for developing high-level machine intelligence",
    outcome_var = d$Q17,
    label_var = d0$Q17,
    output_type = "value_table",
    shown = rep(TRUE, nrow(d)),
    num_missing = 8,
    num_DK = 6, missing_recode = support_hlmi_overall_mean,
    new_values = c(2, 1, 0, -1, -2, 98, 99, 100))
support_hlmi_value_table$num <- support_hlmi_value_table$new_values
support_hlmi_value_sum <- catvar_func(
    outcome = "Support for developing high-level machine intelligence",
    outcome_var = d$Q17,
    label_var = d0$Q17,
    output_type = "num_value",
    shown = rep(TRUE, nrow(d)),
    num_missing = 8,
    num_DK = 6, missing_recode = support_hlmi_overall_mean,
    new_values = c(2, 1, 0, -1, -2, NA, NA, NA))
# Fix the frequency texts
support_hlmi_value_table$Prop_text <- 
  paste0(round(support_hlmi_value_table$Prop*100, digits = 0), "%")
support_hlmi_value_table$Prop_text[support_hlmi_value_table$Prop_text == "0%"] <- "<1%"
# Make the graph
ggplot() +
  geom_bar(data = support_hlmi_value_table, aes(x = num, y = Prop), stat = "identity",
           alpha = 0.55) +
  geom_text(data = support_hlmi_value_table, aes(x = num, 
                                           label = Prop_text), 
            y = 0.02) +
  scale_x_continuous(breaks = support_hlmi_value_table$num[order(support_hlmi_value_table$num)],
    labels = str_wrap(support_hlmi_value_table$labels[order(support_hlmi_value_table$num)], 
                      width = 15)) +
  facet_grid(~group, scales = "free_x", space = "free_x") + theme_bw() +
  geom_point(data = support_hlmi_value_sum, aes(x = num), 
             y = max(support_hlmi_value_table$Prop)+0.02) +
  geom_errorbarh(data = support_hlmi_value_sum, 
                 aes(x = num, xmin = qnorm(0.025)*se + num,
                                       xmax = qnorm(0.975)*se + num,
                                       y = max(support_hlmi_value_table$Prop)+0.02)) + 
  geom_text(data = support_hlmi_value_sum, aes(x = num, label = sum_stat,
                                       y = max(support_hlmi_value_table$Prop)+0.05)) +
  scale_y_continuous(labels = scales::percent, 
                     limits = c(0, max(support_hlmi_value_table$Prop)+0.05)) +
  xlab("Outcomes") + ylab("Percentage of respondents") + 
  ggtitle("Support for developing high-level machine intelligence")

# Correlation between support for developing AI and support for developing HLMI
# AI 
d$Q5_clean <- relabel_var(old_var = d$Q5, old_labels = c(1:6, 8, 9),
                          new_labels = c(2, 1, 0, -1, -2, NA, NA, NA))
d$Q5_missing <- is.na(d$Q5_clean)
d$Q5_clean[is.na(d$Q5_clean)] <- dev_ai_overall_mean
# HLMI
d$Q17_clean <- relabel_var(d$Q17, c(1:6, 8, 9), c(2, 1, 0, -1, -2, NA, NA, NA))
d$Q17_missing <- is.na(d$Q17_clean)
d$Q17_clean[is.na(d$Q17_clean)] <- support_hlmi_overall_mean
cor(d$Q5_clean, d$Q17_clean)
# Linear regression
support_d <- reshape2::melt(d[,c("Q5_clean", "Q17_clean", "survey_weights",
                                 "Q17_missing", "r_id")], 
                            id = c("r_id", "Q17_missing", "survey_weights"))
support_d$variable <- ifelse(support_d$variable == "Q5_clean", "AI", 
                             "High-level machine intelligence")
support_diff <- lm(value ~ variable + scale(Q17_missing) +
                     variable:scale(Q17_missing), 
                   data = support_d, weights = support_d$survey_weights)

# super.cluster.fun<-function(model, cluster)
# {
#   get_confint<-function(model, vcovCL){
#   t<-qt(.975, model$df.residual)
#   ct<-coeftest(model, vcovCL)
#   est<-cbind(ct[,1], ct[,1]-t*ct[,2], ct[,1]+t*ct[,2])
#   colnames(est)<-c("Estimate","LowerCI","UpperCI")
#   return(est)
# }
#   require(multiwayvcov)
#   require(lmtest)
#   vcovCL<-cluster.vcov(model, cluster)
#   
#   coef<-coeftest(model, vcovCL)
#   w<-waldtest(model, vcov = vcovCL, test = "F")
#   ci<-get_confint(model, vcovCL)
#   
#   return(list(coef, w, ci))
# }
# super.cluster.fun(support_diff, support_d$r_id)
```

#### Demographic predictors of support for developing high-level machine intelligence 

The first figure below displays the average support for developing high-level machine intelligence by demographic groups. Males express greater support than females. Those who are employed express greater support than those who are not employed (i.e., retirees, students, or homemakers). Support is positively correlated with family income. Democrats express greater support for developing high-level machine intelligence than Independents. Those with computer science (CS) or engineering degrees or those with CS/programming experience are more likely support developing the technology than those who do not. 

The second figure displays the coefficient plot from a multiple regression model that includes all of the demographic variables. The outcome variable has been standardized, so it has mean 0 and unit variance. Significant predictors correlated with support for developing high-level machine intelligence include:

- identifying as a Democrat or a Republican (versus identifying as an Independent)
- having a family income of more than \$100K annually (versus having a family income of less than \$30K annually)
- having CS or programming experience (versus not having such experience).

Being a female (versus being a male) is a significant predictor of opposition to developing high-level machine intelligence.  

```{r demo_support_hlmi_1, echo=FALSE, fig.height=10, fig.keep='all', fig.width=7, warning=FALSE, cache=TRUE}
demo_support_values <- function(demo, demo_group, output_type = "value_sum") {
  levels_demo <- levels(d[,demo])
  lapply(levels_demo, demo_support, demo = demo, outcome_var = d$Q17, label_var = d0$Q17,
         demo_group = demo_group, output_type = output_type) %>% do.call(what = rbind)
}

# Make the summary statistics
d_value_sum_support <- rbind(
  demo_support_values(demo = "demo_age", demo_group = "Age groups"),
  demo_support_values(demo = "demo_gender", demo_group = "Gender"),
  demo_support_values(demo = "demo_white", demo_group = "Race"),
  demo_support_values(demo = "demo_employ", demo_group = "Employment status"),
  demo_support_values(demo = "demo_income", demo_group = "Income"),
  demo_support_values(demo = "demo_pid3", demo_group = "Political party identification"),
  demo_support_values(demo = "demo_rel", demo_group = "Religion"),
  demo_support_values(demo = "demo_bornagain", demo_group = "Born-again Christian"),
  demo_support_values(demo = "demo_cs", demo_group = "CS or engineering degree"),
  demo_support_values(demo = "demo_prog", demo_group = "CS or programming experience"))
  
# Generate graph
ggplot(data = d_value_sum_support, aes(x = demo_value, y = num, 
                                 ymin = qnorm(0.025) * se + num,
                                 ymax = qnorm(0.975) * se + num)) +
  #geom_hline(yintercept = 0, linetype = 2, alpha = 0.5) +
  geom_pointrange(position = position_dodge(width = 0.9)) + 
  geom_text(aes(label = sum_stat), nudge_y = 0.3, alpha = 0.6, size = 2.5) +
  coord_flip() + 
  scale_x_discrete(labels = function(x) str_wrap(x, width = 30),
    name = "Demographic characteristics (grouped by demographic variable)") + 
  scale_y_continuous(
    name = "Support for developing high-level machine intelligence\n(-2 = Strongly oppose; 2 = Strongly support)") + 
   facet_wrap(~demo_group, scales = "free_y", ncol = 1) +
  ggtitle(str_wrap("Predicting support for developing high-level machine intelligence using demographic characteristics: average support across groups", width = 60)) + 
  theme_bw()  

# Make the value frequency table
d_value_table_support <- rbind(
  demo_support_values(demo = "demo_age", demo_group = "Age groups",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_gender", demo_group = "Gender",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_white", demo_group = "Race",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_employ", demo_group = "Employment status",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_income", demo_group = "Income",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_pid3", demo_group = "Political party identification",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_rel", demo_group = "Religion",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_bornagain", demo_group = "Born-again Christian",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_cs", demo_group = "CS or engineering degree",
                      output_type = "value_table"),
  demo_support_values(demo = "demo_cs", demo_group = "CS or programming experience",
                      output_type = "value_table"))
```

```{r demo_support_hlmi_2, echo=FALSE, fig.height=9, fig.keep='all', fig.width=7, warning=FALSE, cache=TRUE}
# Generate the data for the graph
dev_md <- lm(Q17_clean ~ demo_age + demo_gender + demo_white + demo_employ + 
    demo_pid3 + demo_income + demo_rel + demo_bornagain + demo_cs + 
    demo_prog, data = d, weights = d$survey_weights)
dev_md <- summary(dev_md, robust = TRUE)$coefficients %>% as.data.frame()
names(dev_md) <- c("num", "se", "t", "p")
dev_md$variables <- gsub(pattern = paste0(demo_var, collapse = "|"), replacement = "",
                         x = rownames(dev_md))
# Make the stars
dev_md$stars <- ""
dev_md$stars[dev_md$p < 0.05] <- "*"
dev_md$stars[dev_md$p < 0.01] <- "**"
dev_md$stars[dev_md$p < 0.001] <- "***"
# Make the text
dev_md$new_text <- paste0(roundfunc(dev_md$num), 
                      " (", roundfunc(dev_md$se), ")", dev_md$stars)
dev_md$variables <- factor(dev_md$variables, 
                           levels = rev(dev_md$variables[c(2:nrow(dev_md), 1)]))
# Make a graph
ggplot(data = dev_md, aes(x = variables, y = num, 
                                 ymin = qnorm(0.025) * se + num,
                                 ymax = qnorm(0.975) * se + num)) +
  geom_hline(yintercept = 0, linetype = 2, alpha = 0.5) +
  geom_pointrange(position = position_dodge(width = 0.9)) + 
  geom_text(aes(label = new_text), nudge_x = 0.3, alpha = 0.6) +
  coord_flip() + 
  scale_x_discrete(labels = function(x) str_wrap(x, width = 30),
                         name = "Demographic characteristics") + 
  scale_y_continuous(expand = c(0.05, 0.05), 
    name = "Coefficient estimates (outcome standardized)") + 
  ggtitle(str_wrap("Predicting support for developing high-level machine intelligence using demographic characteristics: results from a multiple regression model that includes all demographic variables", width = 60)) + 
  theme_bw()
```

### Expected outcome of high-level machine intelligence 

#### Procedure

This question seeks to quantify respondents' expected outcome of high-level machine intelligence. Respondents are asked to consider the following:

> Suppose that high-level machine intelligence could be developed one day. How positive or negative do you expect the overall impact of high-level machine intelligence to be on humanity in the long run?

The responses are coded in the following way: 

- 2 = Extremely good
- 1 = On balance good
- 0 = More or less neutral
- -1 = On balance bad
- -2 = Extremely bad, possibly human extinction

#### Results and discussion 

The distribution of responses is displayed in the figure below. The mean outcome and the corresponding standard error are also presented.

Americans, on average, expect the high-level machine intelligence will have a harmful impact on balance. Of the 34% who think the technology will have a harmful impact, 12% said it could be extremely bad, leading to possible human extinction. More than a quarter of Americans think that high-level machine intelligence will be good for humanity, with 5% saying it will be extremely good. Since forecasting the impact of such technology on humanity is highly uncertain, 18% of respondents selected "I don't know." 

```{r expected_outcome, echo=FALSE, results='hide', fig.height=4, fig.keep='all', fig.width=7, warning=FALSE, cache=TRUE}
# Get the overall mean for recoding missing data
outcome_hlmi_overall_mean <- wtd.mean(relabel_var(d$Q18,
                                      c(1:6, 8, 9),
                                      c(2, 1, 0, -1, -2, NA, NA, NA)),
                                      weights = d$survey_weights, na.rm = TRUE)

# Prepare the data
outcome_hlmi_value_table <- catvar_func(
    outcome = 
      "Expected positive or negative impact of high-level machine intelligence on humanity",
    outcome_var = d$Q18,
    label_var = d0$Q18,
    output_type = "value_table",
    shown = rep(TRUE, nrow(d)),
    num_missing = 8,
    num_DK = 6, missing_recode = outcome_hlmi_overall_mean,
    new_values <- c(2, 1, 0, -1, -2, 98, 99, 100))
outcome_hlmi_value_table$num <- support_hlmi_value_table$new_values
outcome_hlmi_value_sum <- catvar_func(
    outcome = 
      "Expected positive or negative impact of high-level machine intelligence on humanity",
    outcome_var = d$Q18,
    label_var = d0$Q18,
    output_type = "num_value",
    shown = rep(TRUE, nrow(d)),
    num_missing = 8,
    num_DK = 6, missing_recode = outcome_hlmi_overall_mean,
    new_values <- c(2, 1, 0, -1, -2, NA, NA, NA))
outcome_hlmi_value_table$Prop_text <- 
  paste0(round(outcome_hlmi_value_table$Prop*100, digits = 0), "%")
outcome_hlmi_value_table$Prop_text[outcome_hlmi_value_table$Prop_text == "0%"] <- "<1%"
# Make the graph
ggplot() +
  geom_bar(data = outcome_hlmi_value_table, aes(x = num, y = Prop), stat = "identity",
           alpha = 0.55) +
  geom_text(data = outcome_hlmi_value_table, aes(x = num, 
                                           label = Prop_text), 
            y = 0.02) +
  scale_x_continuous(breaks = outcome_hlmi_value_table$num[order(outcome_hlmi_value_table$num)],
    labels = str_wrap(outcome_hlmi_value_table$labels[order(outcome_hlmi_value_table$num)], 
                      width = 15)) +
  facet_grid(~group, scales = "free_x", space = "free_x") + theme_bw() +
  geom_point(data = outcome_hlmi_value_sum, aes(x = num), 
             y = max(outcome_hlmi_value_table$Prop)+0.02) +
  geom_errorbarh(data = outcome_hlmi_value_sum, 
                 aes(x = num, xmin = qnorm(0.025)*se + num,
                                       xmax = qnorm(0.975)*se + num,
                                       y = max(outcome_hlmi_value_table$Prop)+0.02)) + 
  geom_text(data = outcome_hlmi_value_sum, aes(x = num, label = sum_stat,
                                       y = max(outcome_hlmi_value_table$Prop)+0.05)) +
  scale_y_continuous(labels = scales::percent, 
                     limits = c(0, max(outcome_hlmi_value_table$Prop)+0.05)) +
  xlab("Outcomes") + ylab("Percentage of respondents") + 
  ggtitle(str_wrap(outcome_hlmi_value_table$outcome, width = 75))

d$Q18_clean <- relabel_var(d$Q18, c(1:6, 8, 9),
                           c(2, 1, 0, -1, -2, NA, NA, NA))
d$Q18_clean[is.na(d$Q18_clean)] <- outcome_hlmi_overall_mean
cor(d$Q17_clean, d$Q18_clean)
```


## Comparing perceptions of global risks

### Procedure

At the beginning of the survey, respondents are asked to consider five out of 15 potential global risks. The purpose of this task is to compare respondents' perception of AI as a catastrophic risk compared with other potential ones. The global risks are selected from the World Economic Forum's ["The Global Risk Report 2018"](https://www.weforum.org/reports/the-global-risks-report-2018). The language used to describe each risk has been simplified so that the average reader can understand it; nevertheless, the substantive content remains the same. We give the following definition for a global risk:

>A "global risk" is an uncertain event or condition that, if it happens, could cause significant negative impact for at least 10 percent of the world's population. That is, at least 1 in 10 people around the world could experience a significant negative impact.

This definition borrows from the [Global Challenges Foundation's](https://api.globalchallenges.org/static/reports/Global-Catastrophic-Risk-Annual-Report-2016.pdf) definition: "an uncertain event or condition that, if it happens, can cause a significant negative impact on at least 10% of the world's population within the next 10 years."

The [15 global risks](#global_risks) respondents are asked to consider include:

- Failure of climate change mitigation and adaptation
- Failure of regional/global governance
- Interstate conflict
- Weapons of mass destruction
- Large-scale involuntary migration
- Spread of infectious diseases
- Water crises
- Food crises 
- Harmful consequences of AI
- Harmful consequences of synthetic biology
- Cyberattacks
- Terrorist attacks
- Global recession
- Extreme weather events
- Natural disasters

After considering each potential global risk, respondents are asked to evaluate the likelihood of it happening globally within ten years and its impact on several countries or industries. For assessing the likelihood, respondents choose from a multiple-choice list (like in the AI governance challenge question). For the analysis, the multiple-choice answers are converted to the median value of each answer choice range. For instance, the "very unlikely" answer is converted to 2.5%. 

For evaluating the potential negative impact of each global risk, respondents are given the following multiple-choice scale:

- 0 = Minimal
- 1 = Minor
- 2 = Moderate
- 3 = Severe
- 4 = Catastrophic

### Results and discussion

The figure below presents the results from respondents' evaluations of global risks. The x-axis is the perceived likelihood of the risk happening globally within ten years. The y-axis is the perceived impact of the risk. The mean perceived likelihood and impact is represented by a dot. The corresponding ellipse contains the 95% confidence region.

In general, Americans perceive all these risks to be impactful: they rate each as having between a moderate (2) and severe (3) negative impact if they were to occur. Americans perceive the use of weapons of mass destruction to be the most impactful -- at the severe level (mean score 3.0 out of 4). Although they do not think this risk as likely as other risks, they still assign it an average of 48% probability of occurring within 10 years. Risks in the upper-right quadrant are perceived to be the most likely as well as the most impactful. These include natural disasters, cyberattacks, and extreme weather events.   

The likelihood of these risks appear excessive. These mean responses suggest (assuming independence) that about eight (out of 15) of these global risks, which will have a “significant negative impact on at least 10% of the world’s population," will take place in the next 10 years. One explanation for this is that it arises from the broad misconception that the world is in a much worse state than it actually is [@pinker2018enlightenment; @rosling2018factfulness]. Another explanation is that it arises as byproduct of respondents interpreting "significant negative impact" in a relatively minimal way, though this interpretation is hard to sustain given the mean severity being between "moderate" and "severe."

The adverse consequences of AI within the next 10 years appear to be a relatively low priority in respondents’ assessment of global risks. It -- along with adverse consequences of synthetic biology -- occupy the lower left corner, which contains what are perceived to be lower-probability, lower-impact risks. These risks are perceived to be as impactful (within the next 10 years) as the failure to address climate change. One interpretation of this is that while Americans' have substantial concerns about the long run impacts of advanced AI, they do not expect these risks to manifest within the next 10 years. We can contrast this interpretation with the above aggregate expectation of a 54% chance of human-level AI within 10 years. One potential interpretation of these two statistics emerges if we assume respondents believe global risks only emerge from human-level AI; in that case, conditional on human-level AI (within 10 years), respondents believe there is about an 80% chance of global risks from AI. 

```{r global_risks, echo=FALSE, fig.height=5, fig.keep='all', fig.width=7, warning=FALSE, cache=TRUE}
global_risks <- c("Failure to address climate change", 
                  "Failure of regional/global governance",
                  "Conflict between major countries", 
                  "Weapons of mass destruction",
                  "Large-scale involuntary migration", 
                  "Spread of infectious diseases", "Water crises",
                  "Food crises",
                  "Harmful consequences of AI", 
                  "Harmful consequences of synthetic biology",
                  "Cyberattacks", "Terrorist attacks",
                  "Global recession", "Extreme weather events", "Natural disasters")


global_risks_clean <- function(risk_num) {
  # Convert multiple-choice outcomes to slider outcomes 
  mc_outcome <- relabel_var(d[,paste0("Q1_", risk_num)], 
                            old_labels = c(1:8, 98, 99), 
                            new_labels = c(mc_p_med, NA, NA, NA))
  # Clean the impact outcomes
  impact <- d[,paste0("Q2_", risk_num)] 
  impact <- relabel_var(impact, old_labels = c(1:6, 8, 9), 
                        new_labels = c(0, 1, 2, 3, 4, NA, NA, NA))
  # Make into a dataframe
  temp <- data.frame(respondent_id = d$r_id,
               risk = global_risks[risk_num],
               survey_weights = d$survey_weights,
                    prob = mc_outcome,
                    impact = impact)
  # Remove the respondents who aren't show the risk
  return(temp[!is.na(d[,paste0("Q1_risks_", risk_num)]) &
                       d[,paste0("Q1_risks_", risk_num)] == 1,])
}

# Clean up the data
gr_clean <- lapply(1:15, global_risks_clean) %>% do.call(what = rbind)
# Recode the missing values
gr_clean$prob_missing <- is.na(gr_clean$prob)
gr_clean$impact_missing <- is.na(gr_clean$impact)
gr_clean$prob[is.na(gr_clean$prob)] <- 
  wtd.mean(gr_clean$prob, weights = gr_clean$survey_weights, na.rm = TRUE)
gr_clean$impact[is.na(gr_clean$impact)] <- 
  wtd.mean(gr_clean$impact, weights = gr_clean$survey_weights, na.rm = TRUE)
# Check the percent missing across risk
gr_sum_missing <- gr_clean %>% group_by(risk) %>% dplyr::summarise(
  prob_missing = mean(prob_missing),
  impact_missing = mean(impact_missing)
)
# Summarize the data
# Helper function
gr_sum_func <- function(risk) {
  md_prob <- if (gr_sum_missing$prob_missing[gr_sum_missing$risk == risk] > 0.1) {
    # If more than 10 percent is missing, then we condition on normalized dummy variable for missingness
      summary(lm(prob ~ scale(prob_missing), data = gr_clean[gr_clean$risk == risk,],
               weights = gr_clean$survey_weights[gr_clean$risk == risk]), 
              robust = TRUE)$coefficients
    } else {
      summary(lm(prob ~ 1, 
                 weights = gr_clean$survey_weights[gr_clean$risk == risk], 
                 data = gr_clean[gr_clean$risk == risk,],
                  robust = TRUE))$coefficients
    }
  md_impact <- if (gr_sum_missing$impact_missing[gr_sum_missing$risk == risk] > 0.1) {
    # If more than 10 percent is missing, then we condition on normalized dummy variable for missingness
      summary(lm(impact ~ scale(impact_missing), 
                 data = gr_clean[gr_clean$risk == risk,],
               weights = gr_clean$survey_weights[gr_clean$risk == risk]), 
              robust = TRUE)$coefficients
    } else {
      summary(lm(impact ~ 1, weights = gr_clean$survey_weights[gr_clean$risk == risk], 
                 data = gr_clean[gr_clean$risk == risk,],
                  robust = TRUE))$coefficients
    }
  data.frame(risk = risk, prob = md_prob[1,1], impact = md_impact[1,1], 
             N = length(gr_clean$respondent_id[gr_clean$risk == risk]))
}
# Run the analysis
gr_sum <- lapply(global_risks, gr_sum_func) %>% do.call(what = rbind)
# Make the confidence ellpises
gr_cr_func <- function(risk, alpha = 0.05, m = 2000) {
  survey_weights <- gr_clean$survey_weights[gr_clean$risk == risk]
  md_prob <- if (gr_sum_missing$prob_missing[gr_sum_missing$risk == risk] > 0.1) {
    # If more than 10 percent is missing, then we condition on normalized dummy variable for missingness
      lm(prob ~ scale(prob_missing), data = gr_clean[gr_clean$risk == risk,],
               weights = survey_weights)
    } else {
      lm(prob ~ 1, 
                 weights = survey_weights, 
                 data = gr_clean[gr_clean$risk == risk,])
    }
  md_impact <- if (gr_sum_missing$impact_missing[gr_sum_missing$risk == risk] > 0.1) {
    # If more than 10 percent is missing, then we condition on normalized dummy variable for missingness
      lm(impact ~ scale(impact_missing), 
                 data = gr_clean[gr_clean$risk == risk,],
               weights = survey_weights)
    } else {
      lm(impact ~ 1, weights = survey_weights, 
                 data = gr_clean[gr_clean$risk == risk,])
    }
  # Generate the variance-covariance matrix using the residuals from our models
  cov_res <- cov(cbind(md_prob$residuals, md_impact$residuals))
  # Generate data for the confidence ellipse
  bivCI_res <- bivCI(s = cov_res, xbar = c(md_prob$coefficients[1],
                                               md_impact$coefficients[1]),
      n = length(survey_weights), alpha = alpha, m = m)
  return(data.frame(risk = risk, prob = bivCI_res$x, impact = bivCI_res$y))
}
gr_cr <- do.call(rbind, lapply(global_risks, gr_cr_func))
# Plot the data
ggplot() + 
  geom_path(data = gr_cr, aes(x = prob/100, y = impact, color = risk), alpha = 0.3) + 
  geom_point(data = gr_sum, aes(x = prob/100, y = impact, color = risk), size = 3) +
  geom_text_repel(data = gr_sum, aes(x = prob/100, y = impact,
                                     label = str_wrap(risk, width = 20)), 
                  size = 3) +
  scale_x_continuous(name = "Likelihood of happening within 10 years (percentage points)",
                     labels = scales::percent) + 
  ylab("Impact (0 = minimal; 4= catastrophic)") +
  theme_bw() + theme(legend.position="none") +
  ggtitle("The public's perceptions of 15 potential global risks")
```

# Appendix: survey test questions 

The text of some important survey questions is included in this Appendix. The Codebook contains the full text of every question in the survey. 

## AI governance challenges {#gov_challenges}

- **Fairness and transparency in AI used in hiring**: Increasingly, employers are using AI to make hiring decisions. AI has the potential to make less biased hiring decisions than humans. But algorithms trained on biased data can lead to lead to hiring practices that discriminate against certain groups. Also, AI used in this application may lack transparency, such that human users do not understand what the algorithm is doing, or why it reaches certain decisions in specific cases.
- **Fairness and transparency in AI used in criminal justice**: Increasingly, the criminal justice system is using AI to make sentencing and parole decisions. AI has the potential to make less biased hiring decisions than humans. But algorithm trained on biased data could lead to discrimination against certain groups. Also, AI used in this application may lack transparency such that human users do not understand what the algorithm is doing, or why it reaches certain decisions in specific cases.
- **Accuracy and transparency in AI used for disease diagnosis**: Increasingly, AI software has been used to diagnose diseases, such as heart disease and cancer. One challenge is to make sure the AI can correctly diagnose those who have the disease and not mistakenly diagnose those who do not have the disease. Another challenge is that AI used in this application may lack transparency such that human users do not understand what the algorithm is doing, or why it reaches certain decisions in specific cases.
- **Protect data privacy**: Algorithms used in AI applications are often trained on vast amounts of personal data, including medical records, social media content, and financial transactions. Some worry that data used to train algorithms are not collected, used, and stored in ways that protect personal privacy.
- **Make sure autonomous vehicles are safe**: Companies are developing self-driving cars and trucks that require little or no input from humans. Some worry about the safety of autonomous vehicles for those riding in them as well as for other vehicles, cyclists, and pedestrians. 
- **Prevent AI from being used to spread fake and harmful content online**: AI has been used by governments, private groups, and individuals to harm or manipulate internet users. For instance, automated “bots” have been used to generate and spread false and/or harmful news stories, audios, and videos. 
- **Prevent AI cyber attacks against governments, companies, organizations, and individuals**: Computer scientists have shown that AI can be used to launch effective cyber attacks. AI could be used to hack into servers to steal sensitive information, shut down critical infrastructures like power grids or hospital networks, or scale up targeted phishing attacks. 
- **Prevent AI-assisted surveillance from violating privacy and civil liberties**: AI can be used to process and analyze large amounts of text, photo, audio, and video data from social media, mobile communications, and CCTV cameras. Some worry that governments, companies, and employers could use AI to increase their surveillance capabilities.
- **Prevent escalation of a U.S.-China AI arms race**: Leading analysts believe that an “AI arms race” is beginning, in which the U.S. and China are investing billions of dollars to develop powerful AI systems for surveillance, autonomous weapons, cyber operations, propaganda, and command and control systems. Some worry that a U.S.-China arms race could lead to extreme dangers. To stay ahead, the U.S. and China may race to deploy advanced military AI systems that they do not fully understand or can control. We could see catastrophic accidents, such as a rapid, automated escalation involving cyber and nuclear weapons.
- **Make sure AI systems are safe, trustworthy, and aligned with human values**: As AI systems become more advanced, they will increasingly make decisions without human input. One potential fear is that AI systems, while performing jobs they are programmed to do, could unintentionally make decisions that go against the values of its human users, such as physically harming people.
- **Ban the use of lethal autonomous weapons (LAWs)**: Lethal autonomous weapons (LAWs) are military robots that can attack targets without control by humans. LAWs could reduce the use of human combatants on the battlefield. But some worry that adoption of LAWs could lead to mass violence. Because they are cheap and easy to produce in bulk, national militaries, terrorists, and other groups could readily deploy LAWs. 
- **Guarantee a good standard of living for those whose lose their jobs to automation**: Some forecast that AI will increasingly be able to do jobs done by humans today. AI could potentially do the jobs of blue-collar workers, like truckers and factory workers, as well as the jobs of white-collar workers, like financial analysts or lawyers. Some worry that in the future, robots and computers can do most of the jobs that are done by humans today. 
- **Prevent critical AI systems failures**: As AI systems become more advanced, they could be used by the military or in critical infrastructure, like power grids, highways, or hospital networks. Some worry that the failure of AI systems or unintentional accidents in these applications could cause 10 percent or more of all humans to die. 

## Description of actors {#actors_appendix}

In the support for developing and managing AI questions, respondents are asked about their trust in various actors. We provide descriptions of actors that are not well-known to the public.

- NATO: NATO is a military alliance that includes 28 countries including most of Europe, as well as the U.S. and Canada. 
- CERN: The European Organization for Nuclear Research, known as CERN, is a European research organization that operates the largest particle physics laboratory in the world.
- OpenAI: Open AI is an AI non-profit organization with backing from tech investors that seeks to develop safe AI.
- AAAI: Association for the Advancement of Artificial Intelligence (AAAI) is a non-government scientific organization that promotes research in, and responsible use of AI.

Furthermore, Partnership on AI is described as "an association of tech companies, academics, and civil society groups."

## U.S.-China arms race survey experiment treatments {#arms_race_exp}

### Nationalism treatment 

Some leaders in the U.S. military and tech industry argue that the U.S. government should invest much more resources in AI research to ensure that the U.S.’s AI capabilities stay ahead of China’s. Furthermore, they argue that the U.S. government should partner with American tech companies to develop advanced AI systems, particularly for military use.

According to a leaked memo produced by a senior National Security Council official, China has "assembled the basic components required for winning the Al arms race...Much like America’s success in the competition for nuclear weapons, China’s 21st Century Manhattan Project sets them on a path to getting there first."

### War risks treatment

Some prominent thinkers are concerned that a U.S.-China arms race could lead to extreme dangers. To stay ahead, the U.S. and China may race to deploy advanced military AI systems that they do not fully understand or can control. We could see catastrophic accidents, such as a rapid, automated escalation involving cyber and nuclear weapons.

"Competition for AI superiority at [the] national level [is the] most likely cause of World War Three," warned Elon Musk, the CEO of Tesla and SpaceX.

### Common humanity treatment

Some prominent thinkers are concerned that a U.S.-China arms race could lead to extreme dangers. To stay ahead, the U.S. and China may race to deploy advanced military AI systems that they do not fully understand or can control. We could see catastrophic accidents, such as a rapid, automated escalation involving cyber and nuclear weapons.

"Unless we learn how to prepare for, and avoid, the potential risks, AI could be the worst event in the history of our civilization. It brings dangers, like powerful autonomous weapons," warned the late Stephen Hawking, one of the world’s most prominent physicists. At the same time, he said that with proper management of the technology, researchers "can create AI for the good of the world."

## Global risks {#global_risks}

- **Failure to address climate change**: Continued failure of governments and businesses to pass effective measures to reduce climate change, protect people, and help those impacted by climate change to adapt. 
- **Failure of regional or global governance**: Regional organizations (e.g., the European Union) or global organizations (e.g., the United Nations) are unable to resolve issues of economic, political, or environmental importance. 
- **Conflict between major countries**: Disputes between major countries that lead to economic, military, cyber, or societal conflicts. 
- **Weapons of mass destruction**: Use of nuclear, chemical, biological or radiological weapons, creating international crises and killing large numbers of people. 
- **Large-scale involuntary migration**: Large-scale involuntary movement of people, such as refugees, caused by conflict, disasters, environmental or economic reasons.
- **Rapid and massive spread of infectious diseases**: The uncontrolled spread of infectious diseases, for instance as a result of resistance to antibiotics, that leads to widespread deaths and economic disruptions. 
- **Water crises**: A large decline in the available quality and quantity of fresh water that harms human health and economic activity. 
- **Food crises**: Large numbers of people are unable to buy or access food. 
Harmful consequences of artificial intelligence (AI): Intended or unintended consequences of artificial intelligence that causes widespread harm to humans, the economy, and the environment. 
- **Harmful consequences of synthetic biology**: Intended or unintended consequences of synthetic biology, such as genetic engineering, that causes widespread harm to humans, the economy, and the environment. 
- **Large-scale cyber attacks**: Large-scale cyber attacks that cause large economic damages, tensions between countries, and widespread loss of trust in the internet.
- **Large-scale terrorist attacks**: Individuals or non-government groups with political or religious goals that cause large numbers of deaths and major material damage. 
- **Global recession**: Economic decline in several major countries that leads to a decrease in income and high unemployment. 
- **Extreme weather events**: Extreme weather events that cause large numbers of deaths as well as damage to property, infrastructure, and the environment. 
- **Major natural disasters**: Earthquakes, volcanic activity, landslides, tsunamis, or geomagnetic storms that that causes large numbers of deaths as well as damage to property, infrastructure, and the environment. 

# Appendix: methodology

The analysis of the real survey data will closely mimic the analysis of the simulated data. Unless otherwise specified, we perform the following procedure:

- Survey weights provided by YouGov are used in our main analysis. For transparency, in the final report, we will perform the analysis without using survey weights. 
- For estimates of summary statistics or coefficients, "don't know" or missing responses are re-coded to the weighted overall mean, unconditional on treatment conditions. Almost all questions have a "don't know" option. If more than 10% of the variable's values are "don't know" or missing, we will include a (standardized) dummy variable for "don't know"/missing in the analysis. For survey experiment questions, we will compare "don't know"/missing rates across experimental conditions. In our final report, we will also check whether "don't know"/missing is correlated with treatment, pre-treatment covariates, and the interactions between treatment and covariates. Our decision is informed by recommendations in the [Standard Operating Procedures for Don Green's Lab at Columbia University](https://github.com/acoppock/Green-Lab-SOP).
- Heteroscedasticity-consistent standard errors are reported.
- Each error bar shows the 95% confidence intervals. Each confidence ellipse shows the 95% confidence region of the bivariate means assuming the two variables are distributed multivariate normal.

# References

